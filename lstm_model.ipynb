{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pycountry\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ProgbarLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfolder = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dfolder + 'merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['year'] >= 1989]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['country', 'deaths', 'state_deaths', 'nonstate_deaths', 'onesided_deaths', 'civilian_deaths']\n",
    "for col in cols:\n",
    "    pct_missing = df[col].isnull().sum() * 100 / len(df)\n",
    "    print(f'{col}: {pct_missing}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_nan_country = df[df['country'].notna()]\n",
    "df_no_nan_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nonstate_deaths'].fillna(0, inplace=True)\n",
    "df['onesided_deaths'].fillna(0, inplace=True)\n",
    "df['civilian_deaths'].fillna(0, inplace=True)\n",
    "df['deaths'].fillna(0, inplace=True)\n",
    "df['state_deaths'].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.MonthYear.dtype\n",
    "df.year.dtype\n",
    "df.month.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'year' and 'month' columns to a datetime type\n",
    "df['Date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "\n",
    "# columns to be filled\n",
    "fill_cols = [col for col in df.columns if 'count' in col]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Iterate through unique 'isocode' values\n",
    "for isocode in df['isocode'].unique():\n",
    "    df_isocode = df[df['isocode'] == isocode].copy()\n",
    "\n",
    "    # Create a new dataframe that covers all months between the min and max 'Date' for this isocode\n",
    "    min_date = df_isocode['Date'].min()\n",
    "    max_date = df_isocode['Date'].max()\n",
    "    all_dates = pd.DataFrame(pd.date_range(min_date, max_date, freq='MS'), columns=['Date'])\n",
    "\n",
    "    # merge onto the existing dataframe\n",
    "    df_isocode = pd.merge(all_dates, df_isocode, on='Date', how='left')\n",
    "\n",
    "    df_isocode['isocode'] = isocode\n",
    "\n",
    "    # forward-fill and then fill any remaining NaNs with 0\n",
    "    df_isocode[fill_cols] = df_isocode[fill_cols].ffill().fillna(0)\n",
    "\n",
    "    # convert the 'date' back to 'year' and 'month'\n",
    "    df_isocode['year'] = df_isocode['Date'].dt.year\n",
    "    df_isocode['month'] = df_isocode['Date'].dt.month\n",
    "\n",
    "    # append to list\n",
    "    dfs.append(df_isocode)\n",
    "\n",
    "# concatenate all DataFrames in the list into a final DataFrame\n",
    "df_final = pd.concat(dfs)\n",
    "\n",
    "df_final.drop(columns=['Date'], inplace=True)\n",
    "\n",
    "# sort final dataframe by 'isocode', 'year' and 'month'\n",
    "df_final.sort_values(['year', 'month', 'isocode'], inplace=True)\n",
    "df_final.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final\n",
    "df_final.drop(['MonthYear'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_name(iso_code):\n",
    "    try:\n",
    "        return pycountry.countries.get(alpha_3=iso_code).name\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "df_final['country'] = df_final['isocode'].apply(get_country_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_final[df_final['country'] == 'Afghanistan']\n",
    "filtered_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every country isocode has the correct number of months in MonthYear. No months are missing.\n",
    "For every country, the months are in ascending order, meaning the data for each isocode is ordered correctly by time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_month_year_sequence(group):\n",
    "    # Create the expected sequence of months and years\n",
    "    min_year, min_month = group[['year', 'month']].iloc[0] # use the first row of each group\n",
    "    max_year, max_month = group[['year', 'month']].iloc[-1] # use the last row of each group\n",
    "\n",
    "    expected_month_years = [(y, m) for y in range(min_year, max_year + 1) for m in range(1, 13)]\n",
    "    \n",
    "    # If there is only one year in the data, filter for months within the min and max range\n",
    "    if min_year == max_year:\n",
    "        expected_month_years = [my for my in expected_month_years if min_month <= my[1] <= max_month]\n",
    "    else:\n",
    "        # If there are multiple years, adjust for the first and last years\n",
    "        expected_month_years = [my for my in expected_month_years if \n",
    "                                not (my[0] == min_year and my[1] < min_month) and \n",
    "                                not (my[0] == max_year and my[1] > max_month)]\n",
    "                                \n",
    "    # Check if the sequence of month-years in the group is equal to the expected sequence\n",
    "    actual_month_years = sorted(list(zip(group['year'], group['month'])))\n",
    "    \n",
    "    if actual_month_years != expected_month_years:\n",
    "        print(f\"Incorrect sequence for isocode: {group['isocode'].iloc[0]}\")\n",
    "        print(f\"Expected: {expected_month_years}\")\n",
    "        print(f\"Actual: {actual_month_years}\")\n",
    "        \n",
    "    return actual_month_years == expected_month_years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each group\n",
    "is_sequence_correct = df_final.groupby('isocode').apply(check_month_year_sequence)\n",
    "\n",
    "# Check if the sequence of month-years is correct for all isocodes\n",
    "assert is_sequence_correct.all(), \"The sequence of month-years is not correct for some isocodes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['date'] = pd.to_datetime(df_final[['year', 'month']].assign(day=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "df_final.set_index('date')['state_deaths'].plot()\n",
    "plt.title('State Deaths over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('State Deaths')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_final['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "# df_final.set_index('date', inplace=True)\n",
    "\n",
    "# # list of all countries\n",
    "# countries = df_final['country'].unique()\n",
    "\n",
    "# # store the lagged features\n",
    "# df_lagged = pd.DataFrame()\n",
    "\n",
    "# # Loop over each country\n",
    "# for country in countries:\n",
    "#     df_country = df[df['country'] == country].copy()\n",
    "#     df_country['deaths_lag'] = df_country['deaths'].shift(1) \n",
    "#     df_lagged = pd.concat([df_lagged, df_country])\n",
    "\n",
    "# # Drop rows with missing values\n",
    "# df_lagged.dropna(subset=['deaths_lag'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.sort_values(['year', 'month', 'isocode'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all event count columns for gov, opp and total events\n",
    "event_cols = ['count_events_{}'.format(i) for i in range(1, 21)]\n",
    "event_cols_gov = ['count_events_{}_gov'.format(i) for i in range(1, 21)]\n",
    "event_cols_opp = ['count_events_{}_opp'.format(i) for i in range(1, 21)]\n",
    "\n",
    "# Compute the total events for each group\n",
    "df_final['total_events'] = df_final[event_cols].sum(axis=1)\n",
    "df_final['total_events_gov'] = df_final[event_cols_gov].sum(axis=1)\n",
    "df_final['total_events_opp'] = df_final[event_cols_opp].sum(axis=1)\n",
    "\n",
    "# Compute the share of each type of event for each group and create new columns\n",
    "for col in event_cols:\n",
    "    df_final['share_events_{}'.format(col)] = df_final[col] / df_final['total_events']\n",
    "    \n",
    "for col in event_cols_gov:\n",
    "    df_final['share_events_{}'.format(col)] = df_final[col] / df_final['total_events_gov']\n",
    "\n",
    "for col in event_cols_opp:\n",
    "    df_final['share_events_{}'.format(col)] = df_final[col] / df_final['total_events_opp']\n",
    "\n",
    "# Drop the original count_events_* columns\n",
    "df_final.drop(columns=event_cols + event_cols_gov + event_cols_opp, inplace=True)\n",
    "df_final = df_final.fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to drop\n",
    "cols_to_drop = ['total_events', 'total_events_gov', 'total_events_opp',\n",
    "                'nonstate_deaths', 'onesided_deaths', 'civilian_deaths', 'state_deaths', 'date', 'country']\n",
    "\n",
    "# Drop the columns\n",
    "df_final = df_final.drop(columns=cols_to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not applying any shifting as my sequence creation in my model handles that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conflict column\n",
    "df_final['conflict'] = df_final['deaths'].apply(lambda x: 1 if x > 100 else 0)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_final = df_final.dropna()\n",
    "\n",
    "# Train and test splits\n",
    "train, test = train_test_split(df_final, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Separate target variable\n",
    "y_train = train['conflict']\n",
    "y_test = test['conflict']\n",
    "train = train.drop(columns=['conflict'])\n",
    "test = test.drop(columns=['conflict'])\n",
    "\n",
    "# One-hot encoding for 'isocode' column\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Fit on train data\n",
    "train_encoded = enc.fit_transform(train['isocode'].values.reshape(-1, 1)).toarray()\n",
    "test_encoded = enc.transform(test['isocode'].values.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Get the list of unique categories from the encoder\n",
    "categories = enc.categories_[0]\n",
    "\n",
    "# Create DataFrame from encoded data, with original column names\n",
    "dfOneHot_train = pd.DataFrame(train_encoded, columns = [\"isocode_\"+str(c) for c in categories])\n",
    "dfOneHot_test = pd.DataFrame(test_encoded, columns = [\"isocode_\"+str(c) for c in categories])\n",
    "\n",
    "# Reset index for concatenation\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "# Concatenate the original dataframe and the one-hot encoded dataframe\n",
    "train = pd.concat([train, dfOneHot_train], axis=1)\n",
    "test = pd.concat([test, dfOneHot_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns to scale. Exclude the one-hot encoded columns, 'year', 'month', and 'conflict' \n",
    "scale_cols = [col for col in train.columns if 'isocode' not in col and col not in ['year', 'month', 'conflict', 'deaths']]\n",
    "\n",
    "# Instantiate the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the train data and transform both train and test data\n",
    "train[scale_cols] = scaler.fit_transform(train[scale_cols])\n",
    "test[scale_cols] = scaler.transform(test[scale_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA might be worth implementing but not working correctly right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Apply PCA on the features\n",
    "# pca = PCA(n_components=0.99)\n",
    "# train_pca = pca.fit_transform(train)\n",
    "# test_pca = pca.transform(test)\n",
    "\n",
    "# # Explained variance ratios\n",
    "# explained_variances = pca.explained_variance_ratio_\n",
    "\n",
    "# # Selected components\n",
    "# component_names = [\"PC\" + str(i) for i in range(1, len(explained_variances)+1)]\n",
    "\n",
    "# # DataFrames with the transformed data and original column names\n",
    "# train = pd.DataFrame(train_pca, columns=component_names)\n",
    "# train['conflict'] = y_train  # append y_train back to the dataframe\n",
    "\n",
    "# test = pd.DataFrame(test_pca, columns=component_names)\n",
    "# test['conflict'] = y_test  # append y_test back to the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning them into numpy arrays to make them easier to work with\n",
    "X_train = train.values\n",
    "X_test = test.values\n",
    "# Append y_train and y_test back to the datasets\n",
    "train['conflict'] = y_train.values\n",
    "test['conflict'] = y_test.values \n",
    "test.drop(['index'], axis=1, inplace=True)\n",
    "train.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "# Iterate over all test data\n",
    "\n",
    "# here we grab the number of unique countries so that we know how many countries to expect at each 'batch', in this case, monthly sequence\n",
    "n_countries = test['isocode'].nunique()\n",
    "n_countries_train= train['isocode'].nunique()\n",
    "\n",
    "train = train.sort_values(['year', 'month', 'isocode'])\n",
    "test = test.sort_values(['year', 'month', 'isocode'])\n",
    "\n",
    "train = train.drop(['isocode'], axis=1)\n",
    "test = test.drop(['isocode'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['conflict'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['conflict'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(input_data, tw, target_column='conflict'):\n",
    "    assert target_column in input_data.columns, f\"The target column must be in the dataframe.\"\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw-1):\n",
    "        train_seq = input_data.iloc[i:i+tw].drop(target_column, axis=1).values\n",
    "        train_label = input_data.iloc[i+tw+1][target_column] # here's the shift of the target \n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return inout_seq\n",
    "\n",
    "sequence_length = 1\n",
    "\n",
    "# Create sequences from the training data\n",
    "train_sequences = create_sequences(train, sequence_length)  # this also creates the target\n",
    "X_train = np.array([seq[0] for seq in train_sequences])\n",
    "y_train = np.array([seq[1] for seq in train_sequences]).flatten() \n",
    "\n",
    "# Reshape X to fit LSTM's expected input shape\n",
    "X_train = X_train.reshape((-1, sequence_length, train.shape[1]-1))\n",
    "\n",
    "# Create sequences from the test data\n",
    "test_sequences = create_sequences(test, sequence_length) # this also creates the target\n",
    "X_test = np.array([seq[0] for seq in test_sequences])\n",
    "y_test = np.array([seq[1] for seq in test_sequences]).flatten() \n",
    "\n",
    "# Reshape X to fit LSTM's expected input shape\n",
    "X_test = X_test.reshape((-1, sequence_length, test.shape[1]-1))\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, dropout=0.2, input_shape=(X_train.shape[1], X_train.shape[2])))  # Single LSTM layer with 32 neurons\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "#progbar_logger = ProgbarLogger()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starting point of the training data stays constant (all_data[:end_of_train]), while the ending point moves forward with each iteration (all_data[:end_of_train+i]). Thus, with each iteration, the model is trained on all the previous data plus some new data, which is the definition of an expanding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate X_train and X_test back into a single DataFrame, for easy shifting between train and test sets\n",
    "all_data = np.concatenate((X_train, X_test), axis=0)\n",
    "all_targets = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "# Initialize arrays to store predictions\n",
    "train_preds = np.zeros_like(y_train)\n",
    "test_preds = np.zeros_like(y_test)\n",
    "\n",
    "# function to calculate class weights\n",
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    total = len(y)\n",
    "    return {cls: (1 / count)*(total)/2.0 for cls, count in counter.items()}\n",
    "\n",
    "# Initialize the end of the train data\n",
    "end_of_train = len(X_train)\n",
    "\n",
    "# Iterate over all test data in monthly batches\n",
    "for i in range(0, len(X_test), n_countries):  # we increase the window one batch at a time\n",
    "    print(f\"Training on window {i//n_countries+1}/{len(X_test)//n_countries}\")\n",
    "    \n",
    "    # Get the current training data and targets\n",
    "    current_data = all_data[:end_of_train+i]\n",
    "    current_targets = all_targets[:end_of_train+i]\n",
    "    \n",
    "    # Remove NaN values\n",
    "    nan_mask = np.isnan(current_targets)\n",
    "    current_data = current_data[~nan_mask]\n",
    "    current_targets = current_targets[~nan_mask]\n",
    "    \n",
    "    # Calculate class weights for current data\n",
    "    class_weights = get_class_weights(current_targets)\n",
    "    \n",
    "    # Fit model on all available training data\n",
    "    history = model.fit(current_data, current_targets, class_weight=class_weights,\n",
    "              epochs=5, batch_size=64, verbose=0, shuffle=False)\n",
    "    \n",
    "    # Predict the next unseen data point for each country\n",
    "    next_predictions = model.predict(all_data[end_of_train+i:end_of_train+i+n_countries])  # we predict the next point for each country\n",
    "    \n",
    "    # Store the predictions\n",
    "    test_preds[i:i+n_countries] = next_predictions.flatten() # storing predictions for all countries\n",
    "\n",
    "# Now the expanding window training and prediction is complete\n",
    "print(f\"Final predictions: {test_preds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "y_true = y_test.flatten() \n",
    "y_pred = test_preds.flatten()\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# Calculate Precision and Recall scores\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "# Plot ROC AUC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(*roc_curve(y_true, y_pred)[:2], label='LSTM Model (AUC = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # Random guessing line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC AUC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label='LSTM Model')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# classes\n",
    "y_pred_classes = (test_preds > 0.5).astype(int)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_classes))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_classes))\n",
    "\n",
    "roc_auc_minority = roc_auc_score(y_test == 1, test_preds)\n",
    "print(\"\\nROC AUC for minority class:\", roc_auc_minority)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "#     n_vars = 1 if type(data) is list else data.shape[1]\n",
    "#     df = pd.DataFrame(data)\n",
    "#     cols, names = list(), list()\n",
    "#     # Input sequence (t-n, ... t-1)\n",
    "#     for i in range(n_in, 0, -1):\n",
    "#         cols.append(df.shift(i))\n",
    "#         names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "#     # Current timestep (t=0)\n",
    "#     cols.append(df)\n",
    "#     names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "#     # Put it all together\n",
    "#     agg = pd.concat(cols, axis=1)\n",
    "#     agg.columns = names\n",
    "#     # Drop rows with missing values\n",
    "#     if dropnan:\n",
    "#         agg.dropna(inplace=True)\n",
    "#     return agg\n",
    "\n",
    "# # Preprocessing the data\n",
    "# train_values = train.drop(columns=['conflict']).values\n",
    "# test_values = test.drop(columns=['conflict']).values\n",
    "\n",
    "# # Define LSTM model\n",
    "# months = 1  \n",
    "# n_features = train_values.shape[1]\n",
    "\n",
    "# train_transformed = series_to_supervised(train_values)\n",
    "# test_transformed = series_to_supervised(test_values)\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(60, return_sequences=True, input_shape=(months, n_features), dropout=0.2))\n",
    "# model.add(LSTM(30, dropout=0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
    "\n",
    "# # Define EarlyStopping callback\n",
    "# early_stop = EarlyStopping(monitor='loss', patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = []\n",
    "\n",
    "# previous_year, previous_month = None, None  # Initialize to None\n",
    "\n",
    "# for i in range(len(test)):\n",
    "\n",
    "#     train = series_to_supervised(train)\n",
    "    \n",
    "#     # split into input and output variables\n",
    "#     train_X, train_y = train.iloc[0, :-1], train.iloc[0, -1]\n",
    "#     test_X, test_y = test.iloc[0, :-1], test.iloc[0, -1]\n",
    "\n",
    "#     # Check if year or month has changed\n",
    "#     current_year, current_month = test.iloc[0]['year'], test.iloc[0]['month']\n",
    "#     if current_year != previous_year or current_month != previous_month:\n",
    "#         print(f'Year: {current_year}  Month: {current_month}')\n",
    "#         previous_year, previous_month = current_year, current_month\n",
    "\n",
    "#     # reshape input to be 3D [samples, timesteps, features]\n",
    "#     train_X = train_X.values.reshape((1, months, n_features))\n",
    "#     test_X = test_X.values.reshape((1, months, n_features))  # Corrected line\n",
    "\n",
    "#     # Convert targets to float32\n",
    "#     train_y = np.array([train_y]).astype('float32')\n",
    "#     test_y = np.array([test_y]).astype('float32')\n",
    "\n",
    "#     # fit model\n",
    "#     history = model.fit(train_X, train_y, epochs=5, batch_size=32, verbose=0, callbacks=[early_stop])\n",
    "\n",
    "#     # make a one-step prediction\n",
    "#     yhat = model.predict(test_X)\n",
    "\n",
    "#     # store predictions\n",
    "#     predictions.append(yhat[0,0])\n",
    "\n",
    "#     # append the first row of test set to the end of the training set\n",
    "#     train = train.append(test.iloc[0], ignore_index=True)\n",
    "\n",
    "#     # then remove the first row of test set\n",
    "#     test = test.iloc[1:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
