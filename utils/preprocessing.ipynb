{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#import datetime as dt\n",
    "from datetime import datetime\n",
    "import pycountry as pc\n",
    "import category_encoders as ce\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# for moths\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "\n",
    "# for clusters\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### important: set train end year (e.g. if 2017, then test data starts from Jan 2018)\n",
    "\n",
    "train_end = 2017\n",
    "unit_of_analyis = 'isocode'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and join data\n",
    "We import the merged data set containing the target (deaths) and the covariates (GDELT event counts) alongside the World Bank's population data.\n",
    "- We ensure both data sets start from 1989. \n",
    "- We ensure both data sets contain the same countries. As part of this process we eliminate several countries that rarely appear in GDELT.\n",
    "- Missing values in the population: for 2022 and 2023 we use the population of 2021, only Palestine is missing data for 1989, so we impute the 1990 population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent_path:  /Users/giovannachaves/Documents/BSE/Master's Thesis/thesis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giovannachaves/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3326: DtypeWarning: Columns (73,74) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: \"/Users/giovannachaves/Documents/BSE/Master's Thesis/thesis/data/WorldBank_pop/wb_pop.csv\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fce61a5ee896>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmerged_og\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/merged.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwb_pop_og\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/WorldBank_pop/wb_pop.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NB in wide format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mwb_pop_og\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 67'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2022'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Indicator Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Indicator Code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mextra_pop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/TWN_ESH.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# We add population estimates for Taiwan and Western Sahara from a separate source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"/Users/giovannachaves/Documents/BSE/Master's Thesis/thesis/data/WorldBank_pop/wb_pop.csv\""
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "parent_path = os.path.abspath(os.path.join(path, os.pardir))\n",
    "\n",
    "print('parent_path: ', parent_path)\n",
    "\n",
    "merged_og = pd.read_csv(os.path.abspath(parent_path + '/data/merged.csv'))\n",
    "wb_pop_og = pd.read_csv(os.path.abspath(parent_path + '/data/WorldBank_pop/wb_pop.csv'), sep=';') # NB in wide format\n",
    "wb_pop_og.drop(columns=['Unnamed: 67', '2022', 'Indicator Name', 'Indicator Code'], inplace=True)\n",
    "extra_pop = pd.read_csv(os.path.abspath(parent_path + '/data/TWN_ESH.csv'), sep=',') # We add population estimates for Taiwan and Western Sahara from a separate source\n",
    "\n",
    "merged_og = merged_og[merged_og['year'] >= 1989]\n",
    "extra_pop = extra_pop[extra_pop['year'] >= 1989]\n",
    "\n",
    "print('countries with pop data: ',wb_pop_og['Country Code'].nunique())\n",
    "print('countries in merged data: ',merged_og['isocode'].nunique())\n",
    "print('extra pop countries: ',extra_pop['isocode'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_og.loc[merged_og['isocode'] == 'MDV', ['year', 'month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged_og.copy()\n",
    "\n",
    "# Because of issue with MonthYear column: create new column with year and month\n",
    "merged['year'] = merged['year'].astype(str)\n",
    "merged['month'] = merged['month'].astype(str)\n",
    "\n",
    "# Pad the month column with leading zeros if needed (e.g., convert '1' to '01')\n",
    "merged['month'] = merged['month'].str.zfill(2)\n",
    "\n",
    "# Concatenate the year and month columns with a '-' &  convert to datetime object\n",
    "merged['month_year'] = merged['year'] + '-' + merged['month']\n",
    "merged['month_year'] = pd.to_datetime(merged['month_year'], format='%Y-%m')\n",
    "\n",
    "\n",
    "# compare month_year and MonthYear\n",
    "merged.MonthYear = pd.to_datetime(merged.MonthYear)\n",
    "\n",
    "print('entires merged:', len(merged))\n",
    "print('date range:', merged.month_year.min(), 'to', merged.month_year.max())\n",
    "# print('date range:', merged.MonthYear.min(), 'to', merged.MonthYear.max())\n",
    "\n",
    "# merged[['month_year', 'MonthYear']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which countries are in the GDELT-UCDP dataset but not in the World Bank population or the extra pop dataset\n",
    "missing_pop_data = set(merged['isocode'].unique()) - set(wb_pop_og['Country Code'].unique()) - set(extra_pop['isocode'].unique())\n",
    "\n",
    "print('Countries without population data (pre drop):', len(missing_pop_data))\n",
    "for isoc in missing_pop_data:\n",
    "    print(pc.countries.get(alpha_3=isoc).name)\n",
    "\n",
    "# Drop rarely mentioned countries (with with <413 Month Entries in GDELT)\n",
    "# exception: 'SSD'\n",
    "isocodes_to_drop = ['ABW', 'AIA', 'AND', 'ASM', 'BVT', 'COK', 'CXR', 'CYM', 'FLK', 'FRO', 'GGY', 'GLP', 'GUF', 'HMD', 'IMN', 'IOT', 'KIR', 'KNA', 'LCA', 'LIE', 'MTQ', 'MYT', 'NFK', 'NIU', 'NRU', 'PCN', 'PLW', 'PYF', 'REU', 'SHN', 'SJM', 'SMR', 'SPM', 'TCA', 'TKL', 'TUV', 'VCT', 'VGB', 'WLF']\n",
    "merged = merged[~merged['isocode'].isin(isocodes_to_drop)]\n",
    "\n",
    "# Check missing counries again\n",
    "missing_pop_data = set(merged['isocode'].unique()) - set(wb_pop_og['Country Code'].unique()) - set(extra_pop['isocode'].unique())\n",
    "\n",
    "print('___________________________________________________________')\n",
    "print('Countries without population data (post drop):', len(missing_pop_data))\n",
    "\n",
    "for isoc in missing_pop_data:\n",
    "    print(pc.countries.get(alpha_3=isoc).name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the remaining countries without population data - no longer having to drop Western Sahara and Taiwan\n",
    "isocodes_to_drop = ['ATA', 'JEY', 'MSR', 'VAT'] #'ESH' 'TWN'\n",
    "merged = merged[~merged['isocode'].isin(isocodes_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After March 2023, no deaths present - so assume not matched to UCDP data and not used\n",
    "for date in ['2023-02-01','2023-03-01', '2023-04-01', '2023-05-01']:\n",
    "    print(f'countries: in {date}: ', len(merged[merged['month_year'] == date]['deaths']))\n",
    "    print('of which nan deaths:      ',merged[merged['month_year'] == date]['deaths'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute rows for missing months\n",
    "We want a complete set of months between the first and the final mention of a country in our dataset. At the end we verify that we have the expected number of months for each country.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_fill = df_merged.copy()\n",
    "df_fill = merged.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_fill' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cac62593ef9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 'year' and 'month' columns to a datetime type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_fill\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_fill\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# columns to be filled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfill_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_fill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'events'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# previously 'counts', but that missed teh normalised total event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_fill' is not defined"
     ]
    }
   ],
   "source": [
    "# 'year' and 'month' columns to a datetime type\n",
    "df_fill['date'] = pd.to_datetime(df_fill[['year', 'month']].assign(day=1))\n",
    "\n",
    "# columns to be filled\n",
    "fill_cols = [col for col in df_fill.columns if 'events' in col] # previously 'counts', but that missed teh normalised total event\n",
    "\n",
    "dfs = []\n",
    "\n",
    "# Iterate through unique 'isocode' values\n",
    "for isocode in df_fill['isocode'].unique():\n",
    "    df_isocode = df_fill[df_fill['isocode'] == isocode].copy()\n",
    "\n",
    "    # Create a new dataframe that covers all months between the min and max 'Date' for this isocode\n",
    "    min_date = df_isocode['date'].min()\n",
    "    max_date = df_isocode['date'].max()\n",
    "    all_dates = pd.DataFrame(pd.date_range(min_date, max_date, freq='MS'), columns=['date'])\n",
    "\n",
    "    # merge onto the existing dataframe\n",
    "    df_isocode = pd.merge(all_dates, df_isocode, on='date', how='left')\n",
    "\n",
    "    df_isocode['isocode'] = isocode\n",
    "\n",
    "    # forward-fill and then fill any remaining NaNs with 0\n",
    "    df_isocode[fill_cols] = df_isocode[fill_cols].ffill().fillna(0)\n",
    "\n",
    "    # convert the 'date' back to 'year' and 'month'\n",
    "    df_isocode['year'] = df_isocode['date'].dt.year\n",
    "    df_isocode['month'] = df_isocode['date'].dt.month\n",
    "\n",
    "    # append to list\n",
    "    dfs.append(df_isocode)\n",
    "\n",
    "# concatenate all DataFrames in the list into a final DataFrame\n",
    "df_filled = pd.concat(dfs)\n",
    "\n",
    "#df_final.drop(columns=['date'], inplace=True)\n",
    "\n",
    "# sort final dataframe by 'isocode', 'year' and 'month'\n",
    "df_filled.sort_values(['year', 'month', 'isocode'], inplace=True)\n",
    "df_filled.fillna(0)\n",
    "\n",
    "df_filled.drop(columns=['MonthYear'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_filled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8fdcb4b95fe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_filled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'isocode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_country_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_filled' is not defined"
     ]
    }
   ],
   "source": [
    "def get_country_name(iso_code):\n",
    "    try:\n",
    "        return pc.countries.get(alpha_3=iso_code).name\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "df_filled['country'] = df_filled['isocode'].apply(get_country_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_month_year_sequence(group):\n",
    "    # Create the expected sequence of months and years\n",
    "    min_year, min_month = group[['year', 'month']].iloc[0] # use the first row of each group\n",
    "    max_year, max_month = group[['year', 'month']].iloc[-1] # use the last row of each group\n",
    "\n",
    "    expected_month_years = [(y, m) for y in range(min_year, max_year + 1) for m in range(1, 13)]\n",
    "    \n",
    "    # If there is only one year in the data, filter for months within the min and max range\n",
    "    if min_year == max_year:\n",
    "        expected_month_years = [my for my in expected_month_years if min_month <= my[1] <= max_month]\n",
    "    else:\n",
    "        # If there are multiple years, adjust for the first and last years\n",
    "        expected_month_years = [my for my in expected_month_years if \n",
    "                                not (my[0] == min_year and my[1] < min_month) and \n",
    "                                not (my[0] == max_year and my[1] > max_month)]\n",
    "                                \n",
    "    # Check if the sequence of month-years in the group is equal to the expected sequence\n",
    "    actual_month_years = sorted(list(zip(group['year'], group['month'])))\n",
    "    \n",
    "    if actual_month_years != expected_month_years:\n",
    "        print(f\"Incorrect sequence for isocode: {group['isocode'].iloc[0]}\")\n",
    "        print(f\"Expected: {expected_month_years}\")\n",
    "        print(f\"Actual: {actual_month_years}\")\n",
    "        \n",
    "    return actual_month_years == expected_month_years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_filled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-65a0088bed6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Apply the function to each group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mis_sequence_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'isocode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_month_year_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Check if the sequence of month-years is correct for all isocodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mis_sequence_correct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The sequence of month-years is not correct for some isocodes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_filled' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply the function to each group\n",
    "is_sequence_correct = df_filled.groupby('isocode').apply(check_month_year_sequence)\n",
    "\n",
    "# Check if the sequence of month-years is correct for all isocodes\n",
    "assert is_sequence_correct.all(), \"The sequence of month-years is not correct for some isocodes\"\n",
    "\n",
    "# check how many rows added\n",
    "print('Rows added:', len(df_filled) - len(df_fill))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melt WB data into long format \n",
    "Fill 2022 and 2023 with 2021 population value and cut off at 1989."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wb_pop_og' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-06e0577ea390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwb_pop_og\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# fill missing complete years (in future to be replaces on basis of growth rate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2022'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2021'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2023'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2021'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wb_pop_og' is not defined"
     ]
    }
   ],
   "source": [
    "pop = wb_pop_og.copy()\n",
    "# fill missing complete years (in future to be replaces on basis of growth rate)\n",
    "pop['2022'] = pop['2021']\n",
    "pop['2023'] = pop['2021']\n",
    "pop.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-73ace7494a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We realised that Palestine is missing population data for 1989 and impute it with 1990 values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmissing_1989\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1989'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmissing_1989\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1989'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing_1989\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1990'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Update the original DataFrame with the imputed values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pop' is not defined"
     ]
    }
   ],
   "source": [
    "# We realised that Palestine is missing population data for 1989 and impute it with 1990 values\n",
    "missing_1989 = pop[pop['1989'].isnull()]\n",
    "missing_1989['1989'] = missing_1989['1990']\n",
    "\n",
    "# Update the original DataFrame with the imputed values\n",
    "pop.update(missing_1989)\n",
    "#pop[pop['1989'].isnull()] # only INX which doesn't seem to be a country\n",
    "county_to_drop = ['INX']\n",
    "pop = pop[~pop['Country Code'].isin(county_to_drop)]\n",
    "\n",
    "# melt population data\n",
    "pop = pd.melt(pop, id_vars=('Country Code', 'Country Name'), var_name='Year', value_name='wb_pop')\n",
    "\n",
    "pop.rename(columns={'Country Code': 'isocode'}, inplace=True)\n",
    "pop.rename(columns={'Year': 'year'}, inplace=True)\n",
    "pop = pop[pop['year'] >= '1989']\n",
    "\n",
    "\n",
    "# checks\n",
    "print(pop.year.min(), pop.year.max())\n",
    "pop.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate missing years for extra population data\n",
    "Western Sahara and Taiwan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years = np.arange(1989, 2024)\n",
    "\n",
    "extra_pop_all = pd.DataFrame()\n",
    "for isocode in extra_pop['isocode'].unique():\n",
    "    # Create a new data frame with all possible years\n",
    "    complete_group = pd.DataFrame({'year': all_years, 'isocode': isocode})\n",
    "    # Merge the original data frame with the complete group based on isocode and year\n",
    "    merged_group = pd.merge(complete_group, extra_pop, on=['isocode', 'year'], how='left')\n",
    "    # Perform linear interpolation within the group\n",
    "    merged_group['population'] = merged_group['population'].interpolate(method='linear')\n",
    "    # Forward fill and backward fill any remaining missing values\n",
    "    merged_group['population'] = merged_group['population'].ffill().bfill()\n",
    "    # Add the complete group to the final data frame\n",
    "    extra_pop_all = pd.concat([extra_pop_all, merged_group])\n",
    "\n",
    "# Sort the complete data frame by isocode and year\n",
    "extra_pop_all.sort_values(['isocode', 'year'], inplace=True)\n",
    "\n",
    "extra_pop_all.rename(columns={'population': 'wb_pop'}, inplace=True)\n",
    "\n",
    "extra_pop_all.drop(columns=['Country Name'], inplace=True)\n",
    "print('Rows added:', len(extra_pop_all) - len(extra_pop))\n",
    "print(extra_pop_all.year.min(), extra_pop_all.year.max())\n",
    "extra_pop_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisations\n",
    "Shows that population distributions change depending on which years are taken into account. Although year-on-year percentage changes are close to zero for most countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_train_start = 1989\n",
    "late_train_start = 2000\n",
    "train_end = 2017\n",
    "test_end = 2022\n",
    "\n",
    "selected_isocodes = [\"AFG\", \"ARE\", \"BDI\", \"BHR\", \"BIH\", \"CYM\", \"DJI\", \"ERI\", \"GNQ\", \"ISR\", \"JOR\", \"KHM\", \"KWT\", \"LBN\", \"LBR\", \"LBY\", \"MAF\", \"MNP\", \"NAM\", \"OMN\", \"QAT\", \"RWA\", \"SGP\", \"SLE\", \"SOM\", \"SSD\", \"SXM\", \"SYR\", \"TCA\", \"XKX\"]\n",
    "\n",
    "# \"AFG\", \"ARE\", \"BDI\", \"BHR\", \"BIH\", \"CYM\", \"DJI\", \"ERI\", \"GNQ\", \"ISR\", \"JOR\", \"KHM\", \"KWT\", \"LBN\", \"LBR\", \"LBY\", \"MAF\", \"MNP\", \"NAM\", \"OMN\", \"QAT\", \"RWA\", \"SGP\", \"SLE\", \"SOM\", \"SSD\", \"SXM\", \"SYR\", \"TCA\", \"XKX\"\n",
    "filtered_data = pop[pop['isocode'].isin(selected_isocodes)]\n",
    "\n",
    "\n",
    "# Filter the data for the two time periods: 1989 vs 2000 until end of train \n",
    "filtered_data['year'] = filtered_data['year'].astype(int)\n",
    "\n",
    "data_1989_train_end = filtered_data[(filtered_data['year'] >= 1989) & (filtered_data['year'] <= train_end)]\n",
    "#data_2000_2022 = filtered_data[(filtered_data['year'] >= 2000) & (filtered_data['year'] <= 2022)]\n",
    "data_2000_train_end = filtered_data[(filtered_data['year'] >= 2000) & (filtered_data['year'] <= train_end)]\n",
    "\n",
    "# Create a list to store the labels for each country\n",
    "labels = []\n",
    "colors = []\n",
    "data_combined = []\n",
    "for isocode in selected_isocodes:\n",
    "    labels.append(isocode)\n",
    "    labels.append('')\n",
    "    colors.extend(['red', 'blue']) #'green'\n",
    "    data_combined.append(data_1989_train_end[data_1989_train_end['isocode'] == isocode]['wb_pop'])\n",
    "    #data_combined.append(data_2000_2022[data_2000_2022['isocode'] == isocode]['wb_pop'])\n",
    "    data_combined.append(data_2000_train_end[data_2000_train_end['isocode'] == isocode]['wb_pop'])\n",
    "\n",
    "# Create the boxplot using matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(data_combined, labels=labels, patch_artist=True,\n",
    "            boxprops=dict(facecolor='white', color='black'),\n",
    "            capprops=dict(color='black'), whiskerprops=dict(color='black'),\n",
    "            flierprops=dict(color='black', markeredgecolor='black'), medianprops=dict(color='black'))\n",
    "\n",
    "for patch, color in zip(plt.boxplot(data_combined, patch_artist=True)['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    \n",
    "# Create a legend for the time period colors\n",
    "legend_elements = [plt.Rectangle((0, 0), 1, 1, color='red', label=f'1989-{train_end}'),\n",
    "                   #plt.Rectangle((0, 0), 1, 1, color='blue', label='2000-2022'),\n",
    "                   plt.Rectangle((0, 0), 1, 1, color='blue', label=f'2000-{train_end}')\n",
    "                   ]\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.title('World Bank Population distributions in different time periods')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('wb_pop')\n",
    "plt.grid(color='grey', linestyle='--', linewidth=0.1)\n",
    "plt.xticks(np.arange(1, len(selected_isocodes) * 2, 2), selected_isocodes, rotation= 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_train_start = 1989\n",
    "late_train_start = 2000\n",
    "train_end = 2017\n",
    "test_end = 2023\n",
    "\n",
    "selected_isocodes = [\"AFG\", \"ARE\", \"BDI\", \"BHR\", \"BIH\", \"CYM\", \"DJI\", \"ERI\", \"GNQ\", \"ISR\", \"JOR\", \"KHM\", \"KWT\", \"LBN\", \"LBR\", \"LBY\", \"MAF\", \"MNP\", \"NAM\", \"OMN\", \"QAT\", \"RWA\", \"SGP\", \"SLE\", \"SOM\", \"SSD\", \"SXM\", \"SYR\", \"TCA\", \"XKX\"]\n",
    "\n",
    "# \"AFG\", \"ARE\", \"BDI\", \"BHR\", \"BIH\", \"CYM\", \"DJI\", \"ERI\", \"GNQ\", \"ISR\", \"JOR\", \"KHM\", \"KWT\", \"LBN\", \"LBR\", \"LBY\", \"MAF\", \"MNP\", \"NAM\", \"OMN\", \"QAT\", \"RWA\", \"SGP\", \"SLE\", \"SOM\", \"SSD\", \"SXM\", \"SYR\", \"TCA\", \"XKX\"\n",
    "filtered_data = pop[pop['isocode'].isin(selected_isocodes)]\n",
    "\n",
    "\n",
    "# Filter the data for the two time periods: 1989 vs 2000 until end of train \n",
    "filtered_data['year'] = filtered_data['year'].astype(int)\n",
    "\n",
    "# 2000-2017\n",
    "data_pop_1 = filtered_data[(filtered_data['year'] >= 2000) & (filtered_data['year'] <= train_end)]\n",
    "#data_2000_2022 = filtered_data[(filtered_data['year'] >= 2000) & (filtered_data['year'] <= 2022)]\n",
    "\n",
    "# 2000-2023\n",
    "data_pop_2 = filtered_data[(filtered_data['year'] >= 2000) & (filtered_data['year'] <= test_end)]\n",
    "\n",
    "# Create a list to store the labels for each country\n",
    "labels = []\n",
    "colors = []\n",
    "data_combined = []\n",
    "for isocode in selected_isocodes:\n",
    "    labels.append(isocode)\n",
    "    labels.append('')\n",
    "    colors.extend(['red', 'blue']) #'green'\n",
    "    data_combined.append(data_pop_1[data_pop_1['isocode'] == isocode]['wb_pop'])\n",
    "    data_combined.append(data_pop_2[data_pop_2['isocode'] == isocode]['wb_pop'])\n",
    "\n",
    "# Create the boxplot using matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(data_combined, labels=labels, patch_artist=True,\n",
    "            boxprops=dict(facecolor='white', color='black'),\n",
    "            capprops=dict(color='black'), whiskerprops=dict(color='black'),\n",
    "            flierprops=dict(color='black', markeredgecolor='black'), medianprops=dict(color='black'))\n",
    "\n",
    "for patch, color in zip(plt.boxplot(data_combined, patch_artist=True)['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    \n",
    "# Create a legend for the time period colors\n",
    "legend_elements = [plt.Rectangle((0, 0), 1, 1, color='red', label=f'{late_train_start}-{train_end}'),\n",
    "                   #plt.Rectangle((0, 0), 1, 1, color='blue', label='2000-2022'),\n",
    "                   plt.Rectangle((0, 0), 1, 1, color='blue', label=f'{train_end}-{test_end}')\n",
    "                   ]\n",
    "plt.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.title('World Bank Population distributions in different time periods') #with a percentage change of more than 5%\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('wb_pop')\n",
    "plt.grid(color='grey', linestyle='--', linewidth=0.1)\n",
    "plt.xticks(np.arange(1, len(selected_isocodes) * 2, 2), selected_isocodes, rotation= 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the year-on-year percentage change for each isocode\n",
    "pop['pct_change'] = pop.groupby('isocode')['wb_pop'].pct_change() * 100\n",
    "\n",
    "# Create a separate line plot for each country\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for isocode, data in pop.groupby('isocode'):\n",
    "    ax.plot(data['year'], data['pct_change'], label=isocode)\n",
    "\n",
    "# Set the legend outside the graph\n",
    "#ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Set the axis labels and title\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Percentage Change in wb_pop')\n",
    "ax.set_title('Year-on-Year Percentage Change in wb_pop for All Isocodes')\n",
    "plt.xticks(np.arange(1, pop.year.nunique()), rotation=45)\n",
    "\n",
    "pop.drop(columns=['pct_change'], inplace=True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: work with averages\n",
    "IMPORTANT: Decide when training data ends!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pop.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop['year'] = pop['year'].astype(int)\n",
    "\n",
    "# Filter the dataframe for the desired years (2020-train_end) / (1989-train_end))\n",
    "pop_2000_to_train_end = pop[pop['year'].between(2000, train_end)]\n",
    "pop_1989_to_train_end = pop[pop['year'].between(1989, train_end)]\n",
    "\n",
    "# Group by 'isocode' and 'Country Name' columns and calculate the average population\n",
    "grouped_2000_to_train_end = pop_2000_to_train_end.groupby(['isocode', 'Country Name'])['wb_pop'].mean().reset_index()\n",
    "grouped_1989_to_train_end = pop_1989_to_train_end.groupby(['isocode', 'Country Name'])['wb_pop'].mean().reset_index()\n",
    "\n",
    "# Merge & rename the columns\n",
    "pop_avrg = pd.merge(grouped_1989_to_train_end, grouped_2000_to_train_end, on=['isocode', 'Country Name'], suffixes=(f'_1989_{train_end}', f'_2000_{train_end}'), how = 'left')\n",
    "pop_avrg.rename(columns={f'wb_pop_2000_{train_end}': f'av_pop_2000_{train_end}', f'wb_pop_1989_{train_end}': f'av_pop_1989_{train_end}'}, inplace=True)\n",
    "\n",
    "########\n",
    "# same for taiwan and western sahara\n",
    "extra_pop_all['year'] = extra_pop_all['year'].astype(int)\n",
    "\n",
    "# Filter the dataframe for the desired years (2020-train_end) / (1989-train_end))\n",
    "extra_pop_2000_to_train_end = extra_pop[extra_pop['year'].between(2000, train_end)]\n",
    "extra_pop_1989_to_train_end = extra_pop[extra_pop['year'].between(1989, train_end)]\n",
    "\n",
    "# Group by 'isocode' and 'Country Name' columns and calculate the average population\n",
    "grouped_2000_to_train_end = extra_pop_2000_to_train_end.groupby(['isocode', 'Country Name'])['population'].mean().reset_index()\n",
    "grouped_1989_to_train_end = extra_pop_1989_to_train_end.groupby(['isocode', 'Country Name'])['population'].mean().reset_index()\n",
    "\n",
    "# Merge & rename the columns\n",
    "extra_pop_avrg = pd.merge(grouped_1989_to_train_end, grouped_2000_to_train_end, on=['isocode', 'Country Name'], suffixes=(f'_1989_{train_end}', f'_2000_{train_end}'))\n",
    "extra_pop_avrg.rename(columns={f'population_2000_{train_end}': f'av_pop_2000_{train_end}', f'population_1989_{train_end}': f'av_pop_1989_{train_end}'}, inplace=True)\n",
    "\n",
    "\n",
    "### join them \n",
    "pop_avrg_all = pd.concat([pop_avrg, extra_pop_avrg], ignore_index=True)\n",
    "pop_avrg_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge population data with GDELT-UCDP data\n",
    "df_merged_av_pop = pd.merge(merged, pop_avrg_all, on=['isocode'], how='left')\n",
    "\n",
    "#df_merged_av_pop.drop(columns=['country'], inplace=True)\n",
    "df_merged_av_pop['year'] = df_merged_av_pop['year'].astype(int)\n",
    "df_pop_average = df_merged_av_pop[df_merged_av_pop['year'] >= 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: work with imputed changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop2 = pop.copy()\n",
    "xpop2 = extra_pop_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join extra data with world bank data\n",
    "all_pop = pd.concat([pop2, xpop2], ignore_index=True)\n",
    "all_pop.sort_values(['year', 'isocode'], inplace=True)\n",
    "\n",
    "# Convert year column to datetime &  set the month and day to 1st of January\n",
    "all_pop['month_year'] = pd.to_datetime(all_pop['year'], format='%Y')\n",
    "all_pop['month_year'] = all_pop['month_year'].dt.strftime('%Y-%m-%d')\n",
    "all_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['month_year'] = pd.to_datetime(merged['month_year'], format='%Y-%m-%d')\n",
    "merged['month_year'] = merged['month_year'].dt.strftime('%Y-%m-%d')\n",
    "all_pop.drop(columns=['year'], inplace=True)\n",
    "trial = pd.merge(merged, all_pop, on=['isocode', 'month_year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.loc[trial['isocode'] == 'TWN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'month_year' column to datetime type\n",
    "trial['month_year'] = pd.to_datetime(trial['month_year'])\n",
    "\n",
    "# Set 'isocode' and 'month_year' as the index\n",
    "trial = trial.set_index(['isocode', 'month_year'])\n",
    "\n",
    "# Function to interpolate missing values within a group\n",
    "def interpolate_group(group):\n",
    "    group['wb_pop'] = group['wb_pop'].interpolate(method='linear')\n",
    "    return group\n",
    "\n",
    "# Apply interpolation within each country group\n",
    "trial = trial.groupby('isocode').apply(interpolate_group)\n",
    "\n",
    "# Reset the index\n",
    "trial = trial.reset_index(drop=False)\n",
    "\n",
    "# # Filter the DataFrame to include only the desired range of months\n",
    "# start_date = pd.to_datetime('2020-01-01')\n",
    "# end_date = pd.to_datetime('2022-12-31')\n",
    "# trial = trial[(trial['month_year'] >= start_date) & (trial['month_year'] <= end_date)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB from Jan 2021 onwards there will be no more changes as 2021 is the final year we have changing population data for\n",
    "trial.loc[trial['isocode'] == 'AFG', ['month_year', 'wb_pop']][-55:-22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of missing population values:', trial['wb_pop'].isnull().sum())\n",
    "print('all missing population values are before', trial[trial['wb_pop'].isnull()].year.max())\n",
    "print('countries: ', trial[trial['wb_pop'].isnull()].isocode.unique())\n",
    "\n",
    "trial.drop(columns=['country'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUBSET Januar 2000 to March 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial['year'] = trial['year'].astype(int)\n",
    "df_pop_interpolated = trial[trial['year'] >= 2000]\n",
    "\n",
    "df_pop_interpolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join all population options in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dimensions are the same\n",
    "print(df_pop_interpolated.shape, df_pop_average.shape)\n",
    "print(df_pop_interpolated.isocode.nunique(), df_pop_average.isocode.nunique())\n",
    "\n",
    "# have both options available\n",
    "pop_options =  df_pop_average.copy()\n",
    "pop_options['intp_pop'] =  df_pop_interpolated['wb_pop']\n",
    "print('nulls:',pop_options.isnull().sum().sum(), '(mainly from deaths and event shares)')\n",
    "pop_options.loc[pop_options['isocode'] == 'AFG', ['isocode', 'month_year', 'intp_pop', f'av_pop_2000_{train_end}', f'av_pop_1989_{train_end}']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep any years before 2023 and within 2023, only keep months that are not April or May\n",
    "pop_options = pop_options[(pop_options['year'] != 2023) | ((pop_options['year'] == 2023) & (~pop_options['month'].isin(['04', '05'])))]\n",
    "\n",
    "pop_options.month_year.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conflict definition(s)\n",
    "Calculate deaths per 100,000 based on different population calculations.\n",
    "\n",
    "Missing UCDP data: means no deaths for tracked dyads occured - assume zero casualties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conf = pop_options.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing deaths with 0\n",
    "death_columns = ['deaths', 'state_deaths', 'nonstate_deaths', 'onesided_deaths', 'civilian_deaths']\n",
    "df_conf[death_columns] = df_conf[death_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate deaths per 100,000 people for different population counts\n",
    "population_versions = [f'av_pop_1989_{train_end}', f'av_pop_2000_{train_end}','intp_pop' ]\n",
    "\n",
    "for wb_pop in population_versions:\n",
    "    df_conf[f'deaths_all_{wb_pop}_pc'] = df_conf['deaths'] /  df_conf[wb_pop] *100_000\n",
    "    #df_conf[f'deaths_state_{wb_pop}_pc'] = df_conf['state_deaths'] /  df_conf[wb_pop] *100_000\n",
    "    \n",
    "\n",
    "#df_conf['deaths_all_pc'] = df_conf['deaths'] /  df_conf['wb_pop'] *100_000\n",
    "#df_conf['deaths_state_pc'] = df_conf['deaths'] /  df_conf['wb_pop'] *100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate conflict dummy variables based on thresholds\n",
    "\n",
    "for wb_pop in population_versions:\n",
    "    df_conf[f'armedconf_{wb_pop}'] = df_conf[f'deaths_all_{wb_pop}_pc']>0.05 # more than 0.05 deaths per 100,000 people, i.e. 2 per 1 million \n",
    "    df_conf[f'civilwar_{wb_pop}'] = df_conf[f'deaths_all_{wb_pop}_pc']>3\n",
    "\n",
    "# df_conf['armedconf'] = df_conf.deaths_all_pc>0.05\n",
    "# df_conf['civilwar'] = df_conf.deaths_all_pc>3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = df_conf[df_conf['Country Name'] == 'Afghanistan']\n",
    "country_df[['Country Name','month_year', f'av_pop_1989_{train_end}', f'av_pop_2000_{train_end}','intp_pop',\n",
    "        f'deaths_all_av_pop_1989_{train_end}_pc', f'deaths_all_av_pop_2000_{train_end}_pc', 'deaths_all_intp_pop_pc',\n",
    "       f'armedconf_av_pop_1989_{train_end}',  f'armedconf_av_pop_2000_{train_end}', 'armedconf_intp_pop',\n",
    "       f'civilwar_av_pop_1989_{train_end}', f'civilwar_av_pop_2000_{train_end}', 'civilwar_intp_pop']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targets = df_conf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_escalation(df):\n",
    "    df['lag_deaths'] = df.groupby('isocode')['deaths'].shift(1)\n",
    "\n",
    "    df['delta_deaths'] = np.where((df['lag_deaths'] == 0) & (df['deaths'] == 0), 0,\n",
    "                                  np.where((df['lag_deaths'] == 0) & (df['deaths'] != 0), np.inf,\n",
    "                                           np.where((df['lag_deaths']).isna() == True, 0,\n",
    "                                                    (df['deaths'] - df['lag_deaths']) / df['lag_deaths'])))\n",
    "\n",
    "    # Group the data by 'isocode' and calculate the 75th percentile of the previous 24 months' delta_deaths\n",
    "    df['threshold'] = df.groupby('isocode')['delta_deaths'].transform(lambda x: x.shift(1).rolling(window=24, min_periods=1).quantile(0.75))\n",
    "    df['threshold'] = df['threshold'].fillna(0)\n",
    "\n",
    "    # Check if the current month's delta_deaths exceeds the threshold or is infinity\n",
    "    df['escalation'] = (df['deaths_choice'] >= 0.05) & ((df['delta_deaths'] > df['threshold']) | (df['delta_deaths'] == np.inf))\n",
    "    df['escalation'] = df['escalation'].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_targets = calculate_escalation(df_targets)\n",
    "\n",
    "df_targets.drop(columns=['lag_deaths', 'delta_deaths', 'threshold'], inplace=True)\n",
    "\n",
    "\n",
    "# true_counts = df_targets['escalation'].sum()\n",
    "# total_counts = df_targets['escalation'].count()\n",
    "# percentage_true = (true_counts / total_counts) * 100\n",
    "\n",
    "# summary_table = pd.DataFrame({'True Count': pd.Series(true_counts), 'Percentage True': pd.Series(percentage_true)})\n",
    "# print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = [f'armedconf_av_pop_1989_{train_end}',  f'armedconf_av_pop_2000_{train_end}', 'armedconf_intp_pop',\n",
    "       f'civilwar_av_pop_1989_{train_end}', f'civilwar_av_pop_2000_{train_end}', 'civilwar_intp_pop', 'escalation']\n",
    "\n",
    "true_counts = df_targets[column_list].sum()\n",
    "total_counts = df_targets[column_list].count()\n",
    "percentage_true = (true_counts / total_counts) * 100\n",
    "\n",
    "summary_table = pd.DataFrame({'True Count': true_counts , 'Percentage True': percentage_true}) #'Column': column_list, \n",
    "print(summary_table)\n",
    "\n",
    "# in current preprocessing: df_og.armedconf.sum() = 7144\n",
    "\n",
    "#print('Missing values:', df_targets.isna().sum().sum() ) #df_targets.loc[df_conf['isocode']=='TWN', f'armedconf_av_pop_1989_{train_end}'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Past deaths and periods of peace\n",
    "Important: decide which population and conflict definition to go with!\n",
    "NB: Hannes used the total number of deaths rather than the deaths per capita\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_past = df_targets.copy()\n",
    "\n",
    "pop_choice = 'intp_pop'\n",
    "deaths_choice = f'deaths_all_{pop_choice}_pc'\n",
    "conflict_choice = f'armedconf_{pop_choice}'\n",
    "\n",
    "\n",
    "### Drop everything that was not chosen\n",
    "\n",
    "not_chosen_pops = [x for x in population_versions if x != pop_choice]\n",
    "\n",
    "for not_picked in not_chosen_pops:\n",
    "    print('dropping :', f'deaths_all_{not_picked}_pc', f'armedconf_{not_picked}', not_picked)\n",
    "    df_past.drop(columns=[f'deaths_all_{not_picked}_pc', f'armedconf_{not_picked}', not_picked], inplace=True)\n",
    "\n",
    "for vers in population_versions:\n",
    "    print('dropping :', f'civilwar_{vers}')\n",
    "    df_past.drop(columns=f'civilwar_{vers}', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding columns for past deaths (rolling sum)\n",
    "\n",
    "lcols = (df_past.groupby(unit_of_analyis)[deaths_choice] # for each country\n",
    "         .transform(lambda x: x.rolling(y, min_periods=1).sum()) # rolling sum of best\n",
    "         .rename('past' + str(y-1)) \n",
    "         for y in [7, 13, 61, 121]) \n",
    "\n",
    "df_past = df_past.join(pd.DataFrame(lcols).transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_peace(x):\n",
    "    # number of periods in peace\n",
    "    x = list(x) \n",
    "    y = []\n",
    "    for n in range(0, len(x)):\n",
    "        if (x[n] == 0) & (n == 0):\n",
    "            y.append(1) # if it starts in peace\n",
    "        elif x[n] == 1:\n",
    "            y.append(0) # reset to 0 if conflict\n",
    "        else:\n",
    "            y.append(y[n-1]+1) # add 1 if peace\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns that count the months since the last time a given type of conflict was present in that country\n",
    "cols = [conflict_choice] # could have several conlfict cut-offs (e.g. Hannes has: 'anyviolence', 'armedconf', 'civilwar')\n",
    "names = [x+'_since' for x in cols] # sincelast variables are called anviolence_dp etc.\n",
    "df_past[names] = (df_past.groupby('isocode')[cols].transform(count_peace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df = df_past[df_past['Country Name'] == 'Afghanistan']\n",
    "country_df[['Country Name', 'month_year', deaths_choice, 'past6', 'past12', 'past60', 'past120', conflict_choice, f'{conflict_choice}_since']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_simple(x: pd.Series, decay: float = 0.8):\n",
    "    # xs is a stock of x inflow with a decay of 0.8\n",
    "    # nans = x.isnull()\n",
    "    x = list(x.fillna(0))\n",
    "    xs = [] \n",
    "    for n in range(len(x)):\n",
    "        if n == 0: \n",
    "            xs.append(x[n]) # stock starts in initial value\n",
    "        else:\n",
    "            xs.append(x[n] + decay * xs[n-1])\n",
    "    \n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_past[deaths_choice].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay = 0.8\n",
    "\n",
    "unit_of_analyis = 'isocode'\n",
    "\n",
    "print(df_past.shape)\n",
    "\n",
    "\n",
    "lcols_novs = (df_past.groupby(unit_of_analyis)\n",
    "         .apply(lambda x: stock_simple(x[deaths_choice], decay=decay))\n",
    "         .explode().reset_index(drop=True)\n",
    "         .rename(f'deaths_stock') for t in range(1,2))\n",
    "\n",
    "\n",
    "temp_df = pd.DataFrame(lcols_novs).transpose()\n",
    "\n",
    "print(temp_df.shape)\n",
    "\n",
    "df_past.sort_values(by=['isocode', 'month_year'], inplace=True)\n",
    "#temp_df\n",
    "\n",
    "df_past.reset_index(drop=True, inplace=True)\n",
    "df_past = df_past.join(temp_df)\n",
    "\n",
    "print(df_past.shape)\n",
    "\n",
    "df_past.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_past.loc[df_past['isocode'] =='AFG', ['month_year', deaths_choice, 'deaths_stock']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_past.sort_values(by=['month_year', 'isocode'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Event shares & normalised total events\n",
    "We also make an assumption for missing values that allows us to impute them all with zeros:\n",
    "\n",
    "Missing GDELT event counts: means nothing happened in that country in that time period that was of sufficiently high profile to be captured by GDELT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which population data to use\n",
    "df_shares = df_past.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_shares.isnull().sum().sum())\n",
    "\n",
    "# fill missing events with with 0\n",
    "#event_count_columns= df_shares.filter(like='event_count').columns.tolist()\n",
    "event_count_columns = [col for col in df_shares.columns if col.startswith('count_events')]\n",
    "df_shares[event_count_columns] = df_shares[event_count_columns].fillna(0)\n",
    "\n",
    "# check all missing values are gone\n",
    "df_shares.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all event count columns for gov, opp and total events\n",
    "event_cols = ['count_events_{}'.format(i) for i in range(1, 21)]\n",
    "#event_cols_gov = ['count_events_{}_gov'.format(i) for i in range(1, 21)]\n",
    "#event_cols_opp = ['count_events_{}_opp'.format(i) for i in range(1, 21)]\n",
    "\n",
    "# Compute the total events for each group\n",
    "df_shares['total_events'] = df_shares[event_cols].sum(axis=1)\n",
    "#df_shares['total_events_gov'] = df_shares[event_cols_gov].sum(axis=1)\n",
    "#df_shares['total_events_opp'] = df_shares[event_cols_opp].sum(axis=1)\n",
    "\n",
    "# Check if will be diving by zero\n",
    "print('min total events:', df_shares['total_events'].min())\n",
    "#print('min total events gov:', df_shares['total_events_gov'].min())\n",
    "#print('min total events opp:', df_shares['total_events_opp'].min())\n",
    "\n",
    "# Compute the share of each type of event for each group and create new columns\n",
    "for col in event_cols:\n",
    "    df_shares['share_events_{}'.format(col)] = df_shares[col] / df_shares['total_events'] *100\n",
    "    \n",
    "# for col in event_cols_gov:\n",
    "#     df_shares['share_events_{}'.format(col)] = df_shares[col] / df_shares['total_events_gov'] *100\n",
    "\n",
    "# for col in event_cols_opp:\n",
    "#     df_shares['share_events_{}'.format(col)] = df_shares[col] / df_shares['total_events_opp'] *100\n",
    "\n",
    "# Fill missing values with 0\n",
    "print('missing values in shares to be filled:', df_shares.isnull().sum().sum())\n",
    "df_shares = df_shares.fillna(0)\n",
    "\n",
    "df_shares.isnull().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 21):\n",
    "    old_column_name = 'share_events_count_events_{}'.format(i)\n",
    "    new_column_name = 'share_events_{}'.format(i)\n",
    "    # old_column_name_gov = 'share_events_count_events_{}_gov'.format(i)\n",
    "    # new_column_name_gov = 'share_events_{}_gov'.format(i)\n",
    "    # old_column_name_opp = 'share_events_count_events_{}_opp'.format(i)\n",
    "    # new_column_name_opp = 'share_events_{}_opp'.format(i)\n",
    "    df_shares.rename(columns={old_column_name: new_column_name}, inplace=True) # , old_column_name_gov: new_column_name_gov, old_column_name_opp: new_column_name_opp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSTEAD of geenrating shares for each _gov and _opp event, we just capture what percentage of events in that month involve _gov and _opp as an actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specific_events = ['state_deaths', 'nonstate_deaths', 'onesided_deaths', 'civilian_deaths']\n",
    "\n",
    "event_cols_gov = ['count_events_{}_gov'.format(i) for i in range(1, 21)]\n",
    "event_cols_opp = ['count_events_{}_opp'.format(i) for i in range(1, 21)]\n",
    "\n",
    "df_shares['events_gov'] = df_shares[event_cols_gov].sum(axis=1)\n",
    "df_shares['events_opp'] = df_shares[event_cols_opp].sum(axis=1)\n",
    "\n",
    "specific_events = ['events_gov', 'events_opp']\n",
    "\n",
    "# Compute the share of each subset of events of the total events - only when not dividing by zero\n",
    "for col in specific_events:\n",
    "    df_shares['share_{}'.format(col)] = np.where((df_shares['total_events'] > 0) & (df_shares[col] > 0),\n",
    "                                                 round(df_shares[col] / df_shares['total_events'] * 100, 2), 0)\n",
    "\n",
    "# Drop the original columns\n",
    "df_shares.drop(columns=specific_events, inplace=True)\n",
    "\n",
    "df_shares.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_shares.share_events_gov.min(), df_shares.share_events_gov.max(), df_shares.share_events_opp.min(), df_shares.share_events_opp.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to check the newly generated share columns sum to 100 (NB: some seem to sum to less that 99.5, possibly due to being floats)\n",
    "new_column_names = ['share_events_{}'.format(i) for i in range(1, 21)]\n",
    "\n",
    "df_shares['sum_share'] = df_shares[new_column_names].sum(axis=1).astype(int)\n",
    "print(df_shares['sum_share'].value_counts())\n",
    "\n",
    "# drop the sum column\n",
    "df_shares = df_shares.drop(columns=['sum_share'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the events (dividing by the yearly totals)\n",
    "\n",
    "# plot yearly totals\n",
    "monthly_totals = df_shares.groupby('month_year')['total_events'].sum()\n",
    "monthly_totals.plot.bar(figsize=(20, 4), title='Monthly totals of GDELT events')\n",
    "\n",
    "# normalise the events (dividing by the yearly totals)\n",
    "df_shares['norm_total_events'] = df_shares.groupby('month_year')['total_events'].transform(lambda x: x / monthly_totals[x.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_date = '2020-02-01'\n",
    "afg_jan_2020 = df_shares.loc[(df_shares['isocode'] == 'AFG') & (df_shares['month_year'] == example_date)]['total_events'].astype(int)\n",
    "all_jan_2020 = monthly_totals[example_date].astype(int)\n",
    "\n",
    "# print('Total events in Afghanistan in January 2020:', afg_jan_2020)\n",
    "# print('Total events in all countries in January 2020:', all_jan_2020)\n",
    "\n",
    "print('Manually normalised total events', afg_jan_2020 / all_jan_2020)\n",
    "print('Normalised total events from dataframe:')\n",
    "df_shares.loc[(df_shares['isocode'] == 'AFG') & (df_shares['month_year'] == example_date)]['norm_total_events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original count_events_* columns and total event columns\n",
    "df_shares.drop(columns=event_cols + event_cols_gov + event_cols_opp, inplace=True)\n",
    "df_shares.drop(columns=['total_events'], inplace=True) #'events_gov', 'events_opp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate stocks of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks = df_shares.copy()\n",
    "\n",
    "df_stocks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example data frame with a single column of twos\n",
    "df = pd.DataFrame({'twos': [2] * 10})\n",
    "\n",
    "# Pass the 'twos' column to the stock_simple function with decay=0.8\n",
    "result = stock_simple(df['twos'], decay=0.8)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#event share stocks\n",
    "decay = 0.8\n",
    "\n",
    "unit_of_analyis = 'isocode'\n",
    "\n",
    "print(df_stocks.shape)\n",
    "\n",
    "lcols_novs = (df_stocks.groupby(unit_of_analyis)\n",
    "        .apply(lambda x: stock_simple(x[f'share_events_{t}'], decay=decay))\n",
    "        .explode().reset_index(drop=True)\n",
    "        .rename(f'event_share_{t}_stock') for t in range(1,21))\n",
    "\n",
    "new_df = pd.DataFrame(lcols_novs).transpose()\n",
    "\n",
    "print(new_df.shape)\n",
    "\n",
    "\n",
    "# sort by country and month to join accurately!\n",
    "df_stocks.sort_values(by=[unit_of_analyis, 'month_year'], inplace=True)\n",
    "\n",
    "df_stocks.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_stocks = df_stocks.join(new_df)\n",
    "\n",
    "# sort back to original order\n",
    "#df_stocks.sort_values(by=[unit_of_analyis, 'month_year'], inplace=True)\n",
    "\n",
    "\n",
    "print(df_stocks.shape)\n",
    "\n",
    "new_df.isna().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks.loc[df_stocks['isocode'] == 'AFG'][['month_year', 'share_events_1', 'event_share_1_stock']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admin1 features\n",
    "\n",
    "Decide between which years to target encode for!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_encode = 2000\n",
    "end_encode = 2017 # i.e. train from 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_stocks.copy()\n",
    "adm1 = pd.read_csv(parent_path + \"/data/final_gdelt_bycountry.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns before 1989 to match the GDELT & UCDP data\n",
    "adm1.drop(adm1.loc[adm1['year']<start_encode].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm1.sort_values(by=[\"isocode\",\"year\",\"month\",\"ActionGeo_ADM1Code\"])\n",
    "\n",
    "number_regions = adm1.groupby([\"isocode\", \"year\", \"month\"]).nunique()[\"ActionGeo_ADM1Code\"].reset_index()\n",
    "number_regions = number_regions.rename({\"ActionGeo_ADM1Code\": \"num_regions\"}, axis=1)\n",
    "number_regions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging with the full dataset\n",
    "df_final['month'] = df_final['month'].astype(int)\n",
    "df_final = pd.merge(df_final, number_regions, on= [\"isocode\", \"year\", \"month\"], how = \"left\") # left_on=[\"isocode\", \"year\", \"month\"], right_on = [\"isocode\", \"year\", \"month\"]\n",
    "\n",
    "\n",
    "# check no zeros\n",
    "(df_final.num_regions.isna().sum()) / len(df_final) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping adm1 that does not have relevant events\n",
    "# The 'fight' column will contain 1 if there's a non-zero value in any of the columns, otherwise 0\n",
    "\n",
    "pre_drop = len(adm1)\n",
    "print('adm1 frame pre drop', pre_drop)\n",
    "\n",
    "adm1['fight'] = ((adm1['count_events_18'].fillna(0) > 0) | \n",
    "                        (adm1['count_events_19'].fillna(0) > 0) | \n",
    "                        (adm1['count_events_20'].fillna(0) > 0)).astype(int)\n",
    "\n",
    "adm1 = adm1[adm1['fight'] == 1] # This will keep only the rows where 'fight' column is equal to 1\n",
    "\n",
    "post_drop = len(adm1)\n",
    "print('adm1 frame post drop', post_drop)\n",
    "print('droped:', pre_drop - post_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adm1['year'] = adm1['year'].astype(int)\n",
    "# adm1['month'] = adm1['month'].astype(int)\n",
    "\n",
    "adm_full = pd.merge(df_final, adm1[['ActionGeo_ADM1Code',\"isocode\", \"month\", \"year\"]], \n",
    "                               left_on=[\"isocode\", \"month\", \"year\"], right_on = [\"isocode\", \"month\", \"year\"], \n",
    "                               how = \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_full.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_adm(df, year_threshold):\n",
    "    # Split the data into train and test based on the year threshold\n",
    "    df_train = df[df['year'] <= year_threshold]\n",
    "    df_test = df[df['year'] > year_threshold]\n",
    "\n",
    "    # Target encoding the regions that have at least 1 event of type 18, 19, or 20\n",
    "    region_encoder = ce.TargetEncoder(smoothing=1.0)\n",
    "    region_encoder.fit(df_train['ActionGeo_ADM1Code'], df_train[deaths_choice]) #['deaths'])\n",
    "    df_train['ActionGeo_ADM1Code'] = region_encoder.transform(df_train['ActionGeo_ADM1Code'], df_train[deaths_choice]) #['deaths'])\n",
    "    df_test['ActionGeo_ADM1Code'] = region_encoder.transform(df_test['ActionGeo_ADM1Code'], df_test[deaths_choice]) #['deaths'])\n",
    "    df = pd.concat([df_train, df_test])\n",
    "\n",
    "    # Getting the maximum, mean, and median regions for each month/year and country\n",
    "    df['Adm1_Max'] = df.groupby(['isocode', 'month', 'year'])['ActionGeo_ADM1Code'].transform('max')\n",
    "    df['Adm1_Mean'] = df.groupby(['isocode', 'month', 'year'])['ActionGeo_ADM1Code'].transform('mean')\n",
    "    df['Adm1_Median'] = df.groupby(['isocode', 'month', 'year'])['ActionGeo_ADM1Code'].transform('median')\n",
    "\n",
    "    df_transform = df.drop_duplicates(['isocode', 'month', 'year']).drop([\"ActionGeo_ADM1Code\"], axis=1)\n",
    "\n",
    "    return df_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admin = encode_adm(adm_full, end_encode) #only keeps years after in train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share of death types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths = df_admin.copy()\n",
    "\n",
    "print('min total events:', df_deaths['deaths'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_deaths = ['state_deaths', 'nonstate_deaths', 'onesided_deaths', 'civilian_deaths']\n",
    "\n",
    "# Compute the share of each subset of deaths of the total deaths - only when not dividing by zero\n",
    "for col in specific_deaths:\n",
    "    df_deaths['share_{}'.format(col)] = np.where((df_deaths['deaths'] > 0) & (df_deaths[col] > 0),\n",
    "                                                 round(df_deaths[col] / df_deaths['deaths'] * 100, 2), 0)\n",
    "\n",
    "# Drop the original columns\n",
    "df_deaths.drop(columns=specific_deaths, inplace=True)\n",
    "\n",
    "df_deaths.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option to scroll through all columns generated after merging population data\n",
    "print((set(df_deaths.columns) - set(pop_options.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclical months feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cyclical = df_deaths.copy()\n",
    "\n",
    "encoder = CyclicalFeatures(variables=['month'], drop_original=False)\n",
    "df_cyclical = encoder.fit_transform(df_cyclical)\n",
    "\n",
    "df_cyclical.loc[df_cyclical['isocode'] == 'ALB', ['month_year', 'month_sin', 'month_cos']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refugee data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refugees = df_cyclical.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_group(group):\n",
    "    group[columns_to_interpolate] = group[columns_to_interpolate].interpolate(method='linear')\n",
    "    return group\n",
    "\n",
    "def add_refugee_flows(preprocessed_df):\n",
    "    \"\"\"\n",
    "    Add refugee flows within each country group and interpolate by month.\n",
    "\n",
    "    Args:\n",
    "        preprocessed_df (pandas.DataFrame): DataFrame containing the preprocessed data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with interpolated refugee flows.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read UNHCR data\n",
    "    df = pd.read_csv(os.path.abspath(parent_path + '/data/UNHCR.csv'))\n",
    "\n",
    "    # Create 'Total' column\n",
    "    df['Total'] = df.iloc[:, 5:9].sum(axis=1)\n",
    "\n",
    "    # Retrieving the list of countries of origin and destination \n",
    "    common_countries_origin = set(df['Country of origin (ISO)']).intersection(set(preprocessed_df['isocode']))\n",
    "    common_countries_destination = set(df['Country of asylum (ISO)']).intersection(set(preprocessed_df['isocode']))\n",
    "\n",
    "    # Aggregating refugee flows at country of origin level\n",
    "    df_origin_agg = df.groupby(['Country of origin (ISO)', 'Year'])['Total'].sum().reset_index()\n",
    "    df_origin_agg.rename(columns={'Total': 'refugees_out'}, inplace=True)\n",
    "\n",
    "    # Aggregating refugee flows at country of destination level\n",
    "    df_destination_agg = df.groupby(['Country of asylum (ISO)', 'Year'])['Total'].sum().reset_index()\n",
    "    df_destination_agg.rename(columns={'Total': 'refugees_in'}, inplace=True)\n",
    "\n",
    "    # Merge aggregated refugee flows with prepross\n",
    "    df_wref = preprocessed_df.merge(df_origin_agg, left_on=['isocode', 'year'], right_on=['Country of origin (ISO)', 'Year'], how='left')\n",
    "    df_wref = df_wref.merge(df_destination_agg, left_on=['isocode', 'year'], right_on=['Country of asylum (ISO)', 'Year'], how='left')\n",
    "    df_wref.drop(['Country of origin (ISO)', 'Country of asylum (ISO)', 'Year_x', 'Year_y'], axis=1, inplace=True)\n",
    "\n",
    "    # Fill NAs before 2023 with 0\n",
    "    df_wref.loc[df_wref['year'] < 2023, ['refugees_out', 'refugees_in']] = \\\n",
    "        df_wref.loc[df_wref['year'] < 2023, ['refugees_out', 'refugees_in']].fillna(0)\n",
    "\n",
    "    # Set values to NA for non-January months\n",
    "    df_wref['month_year'] = pd.to_datetime(df_wref['month_year'], format='%Y-%m-%d')\n",
    "    df_wref.loc[df_wref['month_year'].dt.month != 1, 'refugees_out'] = np.nan\n",
    "    df_wref.loc[df_wref['month_year'].dt.month != 1, 'refugees_in'] = np.nan\n",
    "\n",
    "    # Set 'isocode' and 'month_year' as the index\n",
    "    df_wref = df_wref.set_index(['isocode', 'month_year'])\n",
    "\n",
    "    # List of columns to interpolate\n",
    "    columns_to_interpolate = ['refugees_out', 'refugees_in']\n",
    "\n",
    "    # Apply interpolation within each country group\n",
    "    df_wref = df_wref.groupby('isocode').apply(interpolate_group)\n",
    "\n",
    "    # Reset the index\n",
    "    df_wref = df_wref.reset_index(drop=False)\n",
    "\n",
    "    return df_wref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_interpolate = ['refugees_out', 'refugees_in']\n",
    "\n",
    "df_refugees = add_refugee_flows(df_refugees)\n",
    "print(df_refugees.isnull().sum().sum())\n",
    "df_refugees.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refugees['refugees_out'] = df_refugees['refugees_out'] / df_refugees[pop_choice]\n",
    "df_refugees['refugees_in'] = df_refugees['refugees_in'] / df_refugees[pop_choice]\n",
    "\n",
    "df_refugees.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all missing values are for the Maldives as they only appear in GDELT from 2000 Feb \n",
    "#filtered_df = df_refugees[df_refugees.isnull().any(axis=1)]\n",
    "#filtered_df\n",
    "refugee_nulls = ['refugees_out', 'refugees_in']\n",
    "df_refugees[refugee_nulls] = df_refugees[refugee_nulls].fillna(0)\n",
    "\n",
    "print(df_refugees.isnull().sum().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbouring countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighb_og = df_refugees.copy() #df_cyclical.copy()\n",
    "df_neighb_og.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isocode_dict = {\n",
    "    'AFG': ['IRN', 'PAK', 'TKM', 'UZB', 'TJK', 'CHN'],\n",
    "    'AGO': ['COG', 'COD', 'ZMB', 'NAM'],\n",
    "    'ALB': ['GRC', 'MKD', 'MNE', 'SRB'],\n",
    "    'ARE': ['OMN', 'SAU'],\n",
    "    'ARG': ['BOL', 'BRA', 'CHL', 'PRY', 'URY'],\n",
    "    'ARM': ['AZE', 'GEO', 'IRN', 'TUR'],\n",
    "    'ATG': [],\n",
    "    'AUS': [],\n",
    "    'AUT': ['CZE', 'DEU', 'HUN', 'ITA', 'LIE', 'SVK', 'SVN', 'CHE'],\n",
    "    'AZE': ['ARM', 'GEO', 'IRN', 'RUS', 'TUR'],\n",
    "    'BDI': ['COD', 'RWA', 'TZA'],\n",
    "    'BEL': ['FRA', 'DEU', 'LUX', 'NLD'],\n",
    "    'BEN': ['BFA', 'NER', 'NGA', 'TGO'],\n",
    "    'BFA': ['BEN', 'CIV', 'GHA', 'MLI', 'NER', 'TGO'],\n",
    "    'BGD': ['MMR', 'IND'],\n",
    "    'BGR': ['GRC', 'MKD', 'ROU', 'SRB', 'TUR'],\n",
    "    'BHR': ['KWT', 'SAU'],\n",
    "    'BHS': [],\n",
    "    'BIH': ['HRV', 'MNE', 'SRB'],\n",
    "    'BLR': ['LVA', 'LTU', 'POL', 'RUS', 'UKR'],\n",
    "    'BLZ': ['GTM', 'MEX'],\n",
    "    'BMU': [],\n",
    "    'BOL': ['ARG', 'BRA', 'CHL', 'PRY', 'PER'],\n",
    "    'BRA': ['ARG', 'BOL', 'COL', 'GUF', 'GUY', 'PRY', 'PER', 'SUR', 'URY', 'VEN'],\n",
    "    'BRB': [],\n",
    "    'BRN': ['IDN', 'MYS'],\n",
    "    'BTN': ['CHN', 'IND'],\n",
    "    'BWA': ['NAM', 'ZAF', 'ZWE'],\n",
    "    'CAF': ['CMR', 'COD', 'SDN', 'SSD', 'TCD'],\n",
    "    'CAN': ['USA'],\n",
    "    'CHE': ['AUT', 'DEU', 'FRA', 'ITA', 'LIE'],\n",
    "    'CHL': ['ARG', 'BOL', 'PER'],\n",
    "    'CHN': ['AFG', 'BTN', 'HKG', 'IND', 'KAZ', 'KGZ', 'LAO', 'MAC', 'MNG', 'MMR', 'NPL', 'PRK', 'RUS', 'TJK', 'VNM'],\n",
    "    'CIV': ['BFA', 'GHA', 'GIN', 'LBR', 'MLI'],\n",
    "    'CMR': ['CAF', 'CAF', 'COD', 'GNQ', 'NGA', 'TCD'],\n",
    "    'COD': ['AGO', 'BDI', 'CAF', 'COG', 'RWA', 'SSD', 'TZA', 'UGA', 'ZMB'],\n",
    "    'COG': ['AGO', 'CAF', 'COD', 'GAB'],\n",
    "    'COL': ['BRA', 'ECU', 'PAN', 'PER', 'VEN'],\n",
    "    'COM': [],\n",
    "    'CPV': [],\n",
    "    'CRI': ['NIC', 'PAN'],\n",
    "    'CUB': [],\n",
    "    'CYP': [],\n",
    "    'CZE': ['AUT', 'DEU', 'POL', 'SVK'],\n",
    "    'DEU': ['AUT', 'BEL', 'CZE', 'DNK', 'FRA', 'LUX', 'NLD', 'POL', 'CHE'],\n",
    "    'DJI': ['ETH', 'ERI', 'SOM'],\n",
    "    'DMA': [],\n",
    "    'DNK': ['DEU'],\n",
    "    'DOM': ['HTI'],\n",
    "    'DZA': ['LBY', 'MAR', 'MRT', 'NER', 'TUN', 'ESH'],\n",
    "    'ECU': ['COL', 'PER'],\n",
    "    'EGY': ['ISR', 'LBY', 'SDN'],\n",
    "    'ERI': ['DJI', 'ETH', 'SDN'],\n",
    "    'ESH': ['DZA', 'MRT', 'MAR'],\n",
    "    'ESP': ['AND', 'FRA', 'GIB', 'MAR', 'PRT'],\n",
    "    'EST': ['LVA', 'RUS'],\n",
    "    'ETH': ['DJI', 'ERI', 'KEN', 'SDN', 'SOM', 'SSD'],\n",
    "    'FIN': ['NOR', 'RUS', 'SWE'],\n",
    "    'FJI': [],\n",
    "    'FRA': ['AND', 'BEL', 'DEU', 'ESP', 'ITA', 'LUX', 'MCO', 'CHE'],\n",
    "    'FSM': [],\n",
    "    'GAB': ['CMR', 'COG', 'GNQ'],\n",
    "    'GBR': [],\n",
    "    'GEO': ['ARM', 'AZE', 'RUS', 'TUR'],\n",
    "    'GHA': ['BFA', 'CIV', 'TGO'],\n",
    "    'GIB': ['ESP'],\n",
    "    'GIN': ['CIV', 'GNB', 'LBR', 'MLI', 'SEN', 'SLE'],\n",
    "    'GMB': ['SEN'],\n",
    "    'GNB': ['GIN', 'SEN'],\n",
    "    'GNQ': ['CMR', 'GAB'],\n",
    "    'GRC': ['ALB', 'BGR', 'TUR'],\n",
    "    'GRD': [],\n",
    "    'GRL': [],\n",
    "    'GTM': ['BLZ', 'HND', 'MEX', 'SLV'],\n",
    "    'GUM': [],\n",
    "    'GUY': ['BRA', 'SUR', 'VEN'],\n",
    "    'HKG': ['CHN'],\n",
    "    'HND': ['GTM', 'NIC', 'SLV'],\n",
    "    'HRV': ['BIH', 'HUN', 'MNE', 'SRB', 'SVN'],\n",
    "    'HTI': ['DOM'],\n",
    "    'HUN': ['AUT', 'HRV', 'ROU', 'SRB', 'SVK', 'SVN', 'UKR'],\n",
    "    'IDN': ['BRN', 'MYS', 'PNG', 'TLS'],\n",
    "    'IND': ['AFG', 'BGD', 'BTN', 'CHN', 'MMR', 'NPL', 'PAK'],\n",
    "    'IRL': ['GBR'],\n",
    "    'IRN': ['AFG', 'ARM', 'AZE', 'IRQ', 'PAK', 'TUR', 'TKM'],\n",
    "    'IRQ': ['IRN', 'JOR', 'KWT', 'SAU', 'SYR', 'TUR'],\n",
    "    'ISL': [],\n",
    "    'ISR': ['EGY', 'JOR', 'LBN', 'PSE', 'SYR'],\n",
    "    'ITA': ['AUT', 'FRA', 'SMR', 'SVN', 'CHE', 'VAT'],\n",
    "    'JAM': [],\n",
    "    'JOR': ['IRQ', 'ISR', 'SAU', 'SYR'],\n",
    "    'JPN': [],\n",
    "    'KAZ': ['CHN', 'KGZ', 'RUS', 'TKM', 'UZB'],\n",
    "    'KEN': ['ETH', 'SOM', 'SSD', 'TZA', 'UGA'],\n",
    "    'KGZ': ['CHN', 'KAZ', 'TJK', 'UZB'],\n",
    "    'KHM': ['LAO', 'THA', 'VNM'],\n",
    "    'KOR': ['PRK'],\n",
    "    'KWT': ['IRQ', 'SAU'],\n",
    "    'LAO': ['CHN', 'KHM', 'MMR', 'THA', 'VNM'],\n",
    "    'LBN': ['ISR', 'SYR'],\n",
    "    'LBR': ['GIN', 'CIV', 'GNB', 'SLE'],\n",
    "    'LBY': ['DZA', 'EGY', 'NER', 'SDN', 'TCD', 'TUN'],\n",
    "    'LKA': ['IND'],\n",
    "    'LSO': ['ZAF'],\n",
    "    'LTU': ['BLR', 'LVA', 'POL', 'RUS'],\n",
    "    'LUX': ['BEL', 'DEU', 'FRA'],\n",
    "    'LVA': ['BLR', 'EST', 'LTU', 'RUS'],\n",
    "    'MAC': ['CHN'],\n",
    "    'MAR': ['DZA', 'ESH', 'ESP'],\n",
    "    'MCO': ['FRA'],\n",
    "    'MDA': ['ROU', 'UKR'],\n",
    "    'MDG': [],\n",
    "    'MDV': [],\n",
    "    'MEX': ['BLZ', 'GTM', 'USA'],\n",
    "    'MHL': [],\n",
    "    'MKD': ['ALB', 'BGR', 'GRC', 'SRB'],\n",
    "    'MLI': ['BFA', 'GIN', 'NER', 'SEN'],\n",
    "    'MLT': [],\n",
    "    'MMR': ['BGD', 'CHN', 'IND', 'LAO', 'THA'],\n",
    "    'MNE': ['ALB', 'BIH', 'HRV', 'SRB'],\n",
    "    'MNG': ['CHN', 'RUS'],\n",
    "    'MNP': [],\n",
    "    'MOZ': ['MWI', 'ZAF', 'SWZ', 'TZA', 'ZMB', 'ZWE'],\n",
    "    'MRT': ['DZA', 'MLI', 'SEN'],\n",
    "    'MUS': [],\n",
    "    'MWI': ['MOZ', 'TZA', 'ZMB'],\n",
    "    'MYS': ['BRN', 'IDN', 'THA'],\n",
    "    'NAM': ['AGO', 'BWA', 'ZAF', 'ZMB'],\n",
    "    'NCL': [],\n",
    "    'NER': ['BEN', 'DZA', 'LBY', 'MLI', 'NGA', 'TCD'],\n",
    "    'NGA': ['BEN', 'CMR', 'NER'],\n",
    "    'NIC': ['CRI', 'HND'],\n",
    "    'NLD': ['BEL', 'DEU'],\n",
    "    'NOR': ['FIN', 'RUS', 'SWE'],\n",
    "    'NPL': ['CHN', 'IND'],\n",
    "    'NZL': [],\n",
    "    'OMN': ['ARE', 'SAU', 'YEM'],\n",
    "    'PAK': ['AFG', 'CHN', 'IND', 'IRN'],\n",
    "    'PAN': ['COL', 'CRI'],\n",
    "    'PER': ['BOL', 'BRA', 'CHL', 'COL', 'ECU'],\n",
    "    'PHL': [],\n",
    "    'PNG': ['IDN'],\n",
    "    'POL': ['BLR', 'CZE', 'DEU', 'LTU', 'RUS', 'SVK', 'UKR'],\n",
    "    'PRI': [],\n",
    "    'PRK': ['CHN', 'KOR', 'RUS'],\n",
    "    'PRT': ['ESP'],\n",
    "    'PRY': ['ARG', 'BOL', 'BRA'],\n",
    "    'PSE': ['ISR', 'JOR'],\n",
    "    'QAT': ['SAU'],\n",
    "    'ROU': ['BGR', 'HUN', 'MDA', 'SRB', 'UKR'],\n",
    "    'RUS': ['AZE', 'BLR', 'EST', 'FIN', 'GEO', 'KAZ', 'LVA', 'LTU', 'MNG', 'NOR', 'POL', 'UKR'],\n",
    "    'RWA': ['BDI', 'COD', 'TZA', 'UGA'],\n",
    "    'SAU': ['ARE', 'IRQ', 'JOR', 'KWT', 'OMN', 'QAT', 'UAE', 'YEM'],\n",
    "    'SDN': ['CAF', 'EGY', 'ERI', 'ETH', 'LBY', 'SSD', 'TCD'],\n",
    "    'SEN': ['GIN', 'GNB', 'MRT'],\n",
    "    'SGP': [],\n",
    "    'SLB': [],\n",
    "    'SLE': ['GIN', 'LBR'],\n",
    "    'SLV': ['GTM', 'HND'],\n",
    "    'SOM': ['DJI', 'ETH', 'KEN'],\n",
    "    'SRB': ['ALB', 'BIH', 'BGR', 'HRV', 'HUN', 'KOS', 'MKD', 'MNE', 'ROU'],\n",
    "    'SSD': ['CAF', 'COD', 'ETH', 'KEN', 'SDN', 'UGA'],\n",
    "    'STP': [],\n",
    "    'SUR': ['BRA', 'GUF', 'GUY'],\n",
    "    'SVK': ['AUT', 'CZE', 'HUN', 'POL', 'UKR'],\n",
    "    'SVN': ['AUT', 'HRV', 'HUN', 'ITA'],\n",
    "    'SWE': ['FIN', 'NOR'],\n",
    "    'SWZ': ['MOZ', 'ZAF'],\n",
    "    'SYC': [],\n",
    "    'SYR': ['IRQ', 'ISR', 'JOR', 'LBN', 'TUR'],\n",
    "    'TCD': ['CMR', 'LBY', 'NER', 'SDN'],\n",
    "    'TGO': ['BEN', 'BFA', 'BFA', 'GHA'],\n",
    "    'THA': ['KHM', 'LAO', 'MYS', 'MMR'],\n",
    "    'TJK': ['AFG', 'CHN', 'KGZ', 'UZB'],\n",
    "    'TKM': ['AFG', 'IRN', 'KAZ', 'UZB'],\n",
    "    'TLS': ['IDN'],\n",
    "    'TON': [],\n",
    "    'TTO': [],\n",
    "    'TUN': ['DZA', 'LBY'],\n",
    "    'TUR': ['ARM', 'BGR', 'GEO', 'GRC', 'IRN', 'IRQ', 'SYR'],\n",
    "    'TWN': [],\n",
    "    'TZA': ['BDI', 'COD', 'KEN', 'MWI', 'MOZ', 'RWA', 'UGA', 'ZMB'],\n",
    "    'UGA': ['COD', 'KEN', 'RWA', 'SSD', 'TZA'],\n",
    "    'UKR': ['BLR', 'HUN', 'MDA', 'POL', 'ROU', 'RUS', 'SVK'],\n",
    "    'URY': ['ARG', 'BRA'],\n",
    "    'USA': ['CAN', 'MEX'],\n",
    "    'UZB': ['AFG', 'KAZ', 'KGZ', 'TJK', 'TKM'],\n",
    "    'VEN': ['BRA', 'COL', 'GUY'],\n",
    "    'VIR': [],\n",
    "    'VNM': ['CHN', 'KHM', 'LAO'],\n",
    "    'VUT': [],\n",
    "    'WSM': [],\n",
    "    'XKX': ['ALB', 'MNE', 'MKD', 'SRB'],\n",
    "    'YEM': ['OMN', 'SAU'],\n",
    "    'ZAF': ['BWA', 'LSO', 'MOZ', 'NAM', 'SWZ', 'ZWE'],\n",
    "    'ZMB': ['AGO', 'MOZ', 'MWI', 'NAM', 'TZA', 'ZWE'],\n",
    "    'ZWE': ['MOZ', 'ZAF', 'ZMB']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(os.path.abspath(parent_path + '/data/preprocessed_df.csv'))\n",
    "\n",
    "df_neighb = df_neighb_og.merge(pd.DataFrame(isocode_dict.items(), columns=['isocode', 'neighbors']), on='isocode', how='left')\n",
    "\n",
    "def check_neighbor_conflict(row, df):\n",
    "    \"\"\"\n",
    "    Check if at least one neighbor is in conflict based on the row of the dataframe.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): Row of the dataframe containing country information.\n",
    "        df (pd.DataFrame): The entire dataframe containing country and neighbor information.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if at least one neighbor is in conflict, False otherwise.\n",
    "    \"\"\"\n",
    "    neighbors = row['neighbors']\n",
    "    if isinstance(neighbors, list):\n",
    "        neighbor_status = df[\n",
    "            (df['isocode'].isin(neighbors)) &\n",
    "            (df['month_year'] == row['month_year'])\n",
    "        ][conflict_choice]\n",
    "        if len(neighbor_status) > 0 and neighbor_status.any():\n",
    "            return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function row-wise to create the new column\n",
    "df_neighb['neighbor_conflict'] = df_neighb.apply(check_neighbor_conflict, axis=1, df=df_neighb)\n",
    "df_neighb.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = df_neighb.copy()\n",
    "\n",
    "# ensure we only encode the clusters on the basis of the training data\n",
    "train_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters_train = df_clusters.loc[df_clusters['year'] <= train_end]\n",
    "\n",
    "print(df_clusters_train.month_year.min(), df_clusters_train.month_year.max())\n",
    "\n",
    "## group columns\n",
    "\n",
    "event_share_cols = df_clusters_train.filter(like='share_events').columns.tolist()  #event_share_cols = ['share_events_{}'.format(i) for i in range(1, 21)] \n",
    "event_stock_cols = ['event_share_{}_stock'.format(i) for i in range(1, 21)] \n",
    "#death_share_cols = df_clusters.filter(like='_deaths').columns.tolist() # state, nonstate, one-sided, civilians\n",
    "#past_cols = ['deaths_stock'] #, 'past6', 'past12',\t'past60', 'past120']\n",
    "\n",
    "#peace_count = conflict_choice + '_since'\n",
    "#adm1_cols = ['Adm1_Median', 'Adm1_Mean']\n",
    "\n",
    "# how to aggregate them\n",
    "mean_max =  [deaths_choice]   #+ death_share_cols  + adm1_cols + + [peace_count] + ['deaths'] + past_cols \n",
    "#max_only = ['num_regions'] + ['Adm1_Max']\n",
    "mean_only = [conflict_choice] + ['norm_total_events'] #+ ['Adm1_Mean'] \n",
    "median_only = event_stock_cols\n",
    "all_aggs = event_share_cols\n",
    "\n",
    "cols_not_used = set(df_clusters_train.columns.tolist()) - set(mean_max + max_only + mean_only + median_only + all_aggs)\n",
    "cols_not_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dictionaries to store the aggregation functions, which are dynamically generated for each feature column\n",
    "agg_functions_mean_max = {}\n",
    "agg_functions_max_only = {}\n",
    "agg_functions_mean_only = {}\n",
    "agg_functions_median_only = {}\n",
    "agg_functions_all_aggs = {}\n",
    "\n",
    "for column in mean_max:\n",
    "    agg_functions_mean_max[column] = ['mean', 'max'] #median\n",
    "\n",
    "# for column in max_only:\n",
    "#     agg_functions_max_only[column] = ['max']\n",
    "\n",
    "for column in mean_only:\n",
    "    agg_functions_mean_only[column] = ['mean']\n",
    "\n",
    "for column in all_aggs:\n",
    "    agg_functions_median_only[column] = ['median']\n",
    "\n",
    "for column in all_aggs:\n",
    "    agg_functions_all_aggs[column] = ['mean', 'min', 'max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data = (df_clusters_train.groupby(unit_of_analyis)\n",
    "                .agg({**agg_functions_mean_max,\n",
    "                      #**agg_functions_max_only,\n",
    "                      **agg_functions_mean_only,\n",
    "                      **agg_functions_median_only,\n",
    "                      **agg_functions_all_aggs}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analyis to reduce dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = aggregated_data.copy()\n",
    "country_labels = X.index\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    "\n",
    "n_components = 3  # Set the desired number of components\n",
    "X_selected = X_pca[:, :n_components]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters based on principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(X_selected, columns=['PC1', 'PC2', 'PC3'])  # Adjust column names as needed\n",
    "pca_df['country'] = country_labels  # Add the country labels column\n",
    "pca_df.set_index('country', inplace=True)  # Set the country column as the index\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slelect columns for clustering\n",
    "data_pca = pca_df\n",
    "\n",
    "# normalise data\n",
    "scaler = StandardScaler()\n",
    "normalized_data_pca = scaler.fit_transform(data_pca)\n",
    "\n",
    "# pick cluster range\n",
    "k_values = [5,10,15,20,30, 40] #range(1, 20)\n",
    "\n",
    "# initialize list to store inertia values\n",
    "inertia_values = []\n",
    "\n",
    "#perform k-means clustering for each value of k\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=1000, init = 'k-means++')\n",
    "    kmeans.fit(normalized_data_pca)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# plot the elbow curve\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters based on the elbow point\n",
    "optimal_k = 15\n",
    "\n",
    "# Perform k-means clustering with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10, max_iter=1000, init = 'k-means++')\n",
    "kmeans.fit(normalized_data_pca)\n",
    "\n",
    "# Assign observations to clusters\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "data_pca['cluster'] = cluster_labels\n",
    "\n",
    "# Function to get country name from ISO code\n",
    "def get_country_name(iso_code):\n",
    "    try:\n",
    "        country = pc.countries.get(alpha_3=iso_code)\n",
    "        return country.name\n",
    "    except AttributeError:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Generate column of country names\n",
    "data_pca['isocode'] = data_pca.index\n",
    "data_pca['Country Name'] = data_pca['isocode'].apply(get_country_name)\n",
    "\n",
    "## Clusters based on PCA version of aggregated features\n",
    "# cluster_names = []\n",
    "# for i in range(0,15):\n",
    "#     print(f'cluster {i}' , data_pca.loc[data_pca['Cluster'] == i].index.tolist())\n",
    "\n",
    "cluster_names = []\n",
    "for i in range(0,15):\n",
    "    print(f'cluster {i}' , data_pca.loc[data_pca['cluster'] == i, 'Country Name'].tolist())\n",
    "\n",
    "\n",
    "# save values in a dictionary to apply to the original data\n",
    "cluster_dict = data_pca.groupby('cluster')[unit_of_analyis].apply(list).to_dict()\n",
    "print('cluster dictionary example', cluster_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a color palette with 15 distinct colors\n",
    "num_clusters = data_pca['cluster'].nunique()\n",
    "palette = sns.color_palette('hsv', num_clusters)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for index, row in data_pca.iterrows():\n",
    "    cluster_color = palette[row['cluster'] - 1]\n",
    "    ax.scatter(row['PC1'], row['PC2'], row['PC3'], c=[cluster_color], label=row[unit_of_analyis])\n",
    "    ax.text(row['PC1'], row['PC2'], row['PC3'], row[unit_of_analyis], color='black', fontsize=10, ha='center', va='center')\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "legend_elements = []\n",
    "for cluster_label, color in enumerate(palette, start=1):\n",
    "    legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, label=f'Cluster {cluster_label}'))\n",
    "\n",
    "# Place the legend outside the graph\n",
    "ax.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters_joined = pd.merge(df_clusters, data_pca[[unit_of_analyis, 'cluster']], on= unit_of_analyis, how='left')\n",
    "\n",
    "print(df_clusters_joined.shape, df_clusters.shape, df_clusters_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clusters_joined.shape, df_clusters.shape)\n",
    "df_clusters_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_bin = ce.BinaryEncoder(cols = ['cluster'])\n",
    "X = df_clusters_joined['cluster']\n",
    "cluster_enc = ce_bin.fit_transform(X)\n",
    "\n",
    "print(cluster_enc.shape, df_clusters_joined.shape)\n",
    "\n",
    "df_clusters = pd.concat([df_clusters_joined, cluster_enc], axis=1)\n",
    "df_clusters.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Country Encoder (for tree-based models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cntry_enc = df_clusters.copy()\n",
    "\n",
    "ce_bin = ce.BinaryEncoder(cols = [unit_of_analyis])\n",
    "X = df_cntry_enc[unit_of_analyis]\n",
    "cntry_enc = ce_bin.fit_transform(X)\n",
    "\n",
    "print(cntry_enc.shape, df_cntry_enc.shape)\n",
    "\n",
    "df_cntry_enc = pd.concat([df_cntry_enc, cntry_enc], axis=1)\n",
    "df_cntry_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targets within a given period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra_targets = df_cntry_enc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target_regr(df: pd.DataFrame, shifters: dict, target: str):\n",
    "    \"\"\"\n",
    "    Generate shifted variables and calculate the maximum for multiple shifters.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        shifters (dict): A dictionary specifying the shifters.\n",
    "            Example: {3: 'w3_', 6: 'w6_'} for 2 shifters with prefixes 'w3_' and 'w6_'.\n",
    "        target (str): The column name of the target variable.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with the maximum for each shifter.\n",
    "\n",
    "    \"\"\"\n",
    "    for shifter, prefix in shifters.items():\n",
    "        # Loop through each period and generate the shift variables\n",
    "        for i in range(1, shifter + 1):\n",
    "            col_name = f'{prefix}{target}{i}'\n",
    "            df[col_name] = df.groupby('isocode')[target].shift(-i)\n",
    "\n",
    "        # Take the maximum for t periods forward and create the new variable\n",
    "        avg_col_name = f'{prefix}target_regr'\n",
    "        df[avg_col_name] = df[[f'{prefix}{target}{i}' for i in range(1, shifter + 1)]].mean(axis=1, skipna=False)\n",
    "\n",
    "        # Drop the shift variables\n",
    "        df = df.drop(columns=[f'{prefix}{target}{i}' for i in range(1, shifter + 1)])\n",
    "\n",
    "        # Shift the resulting column by 1 so that Luis can shift back in LSTM\n",
    "        df[avg_col_name] = df[avg_col_name].shift(1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_target_clsf(df: pd.DataFrame, shifters: dict, target: str):\n",
    "    \"\"\"\n",
    "    Generate shifted variables and calculate the maximum for multiple shifters.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        shifters (dict): A dictionary specifying the shifters.\n",
    "            Example: {3: 'w3_', 6: 'w6_'} for 2 shifters with prefixes 'w3_' and 'w6_'.\n",
    "        target (str): The column name of the target variable.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with the maximum for each shifter.\n",
    "\n",
    "    \"\"\"\n",
    "    for shifter, prefix in shifters.items():\n",
    "        # Loop through each period and generate the shift variables\n",
    "        for i in range(1, shifter + 1):\n",
    "            col_name = f'{prefix}{target}{i}'\n",
    "            df[col_name] = df.groupby('isocode')[target].shift(-i)\n",
    "\n",
    "        # Take the maximum for t periods forward and create the new variable\n",
    "        max_col_name = f'{prefix}target_clsf'\n",
    "        df[max_col_name] = df[[f'{prefix}{target}{i}' for i in range(1, shifter + 1)]].max(axis=1, skipna=False)\n",
    "\n",
    "        # Drop the shift variables\n",
    "\n",
    "        df = df.drop(columns=[f'{prefix}{target}{i}' for i in range(1, shifter + 1)])\n",
    "\n",
    "        # Shift the resulting column by 1 so that Luis can shift back in LSTM\n",
    "        df[max_col_name] = df[max_col_name].shift(1)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifters = {3: 'f3_', 6: 'f6_'}\n",
    "\n",
    "\n",
    "df_extra_targets = make_target_clsf(df_extra_targets, shifters,  'escalation') #'clsf')\n",
    "\n",
    "df_extra_targets.rename(columns={'f3_target_clsf': 'f3_target_escl', 'f6_target_clsf': 'f6_target_escl'}, inplace=True)\n",
    "\n",
    "df_extra_targets = make_target_clsf(df_extra_targets, shifters,  conflict_choice) #'clsf')\n",
    "df_extra_targets = make_target_regr(df_extra_targets, shifters, deaths_choice) #'regr')\n",
    "\n",
    "df_extra_targets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_extra_targets.copy()\n",
    "\n",
    "# reminder of chosen variables - then overwrite them with generic versions\n",
    "print('pop choice:', pop_choice)\n",
    "print('conflict choice:', conflict_choice)\n",
    "print('deaths choice:', deaths_choice)\n",
    "\n",
    "df_final.rename(columns={deaths_choice: 'deaths_all_pc', conflict_choice: 'armedconf', 'armedconf_intp_pop_since': 'armedconf_since'}, inplace=True)\n",
    "\n",
    "# drop columns we do not use for models\n",
    "df_final.drop(columns=['country', pop_choice, 'MonthYear', 'deaths', 'neighbors', 'cluster'], inplace=True) #'Country Name'\n",
    "\n",
    "\n",
    "\n",
    "# Check date range\n",
    "print(df_final.month_year.min(), df_final.month_year.max())\n",
    "print('nan', df_final.isna().sum().sum())\n",
    "\n",
    "df_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = df_final[df_final.isna().any(axis=1)]\n",
    "filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(path + \"/preprocessed_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "df_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
